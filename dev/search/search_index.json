{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Bookshelf","text":"<p><code>bookshelf</code> is one way Climate Resource reuses datasets across projects.</p> <p>Key info : </p> <p>PyPI : </p> <p>Tests : </p> <p>Other info : </p> <p>The <code>bookshelf</code> represents a shared collection of curated datasets or <code>Books</code>. Each <code>Book</code> is a preprocessed, versioned dataset including the notebooks used to produce it. As the underlying datasets or processing are updated, new <code>Books</code> can be created (with an updated version in the case of new data or edition if the processing changed). A single dataset may produce multiple <code>Resources</code> if different representations are useful. These <code>Books</code> can be deployed to a shared <code>Bookshelf</code>so that they are accessible by other users.</p> <p>Users are able to use specific <code>Books</code> within other projects. The dataset and associated metadata is fetched and cached locally. Specific versions of <code>Books</code> can also be pinned for reproducibility purposes.</p> <p>This repository contains the notebooks that are used to generate the <code>Books</code> as well as a CLI tool for managing these datasets.</p> <p>This is a prototype and will likely change in future. Other potential ideas:</p> <ul> <li>Deployed data are made available via <code>api.climateresource.com.au</code> so that   they can be consumed queried smartly</li> <li>Simple web page to allow querying the data</li> </ul> <p>Each Book consists of a datapackage description of the metadata. This datapackage contains the associated <code>Resources</code> and their hashes. Each <code>Resource</code> is fetched when it is first used and then cached for later use.</p>"},{"location":"#where-to-next","title":"Where to next?","text":"<p>If you want to use the tool to create input4MIPs files, we recommend going to our how-to guides. Some other potential points of interest:</p> <ul> <li>Getting Started instructions: Getting Started</li> <li>(How-to guides, just in case you missed it the first time: How-to...)</li> <li>The command-line interface's documentation: CLI</li> <li>The full API docs: API reference</li> </ul>"},{"location":"NAVIGATION/","title":"NAVIGATION","text":"<ul> <li>Bookshelf</li> <li>Getting Started</li> <li>Configuration</li> <li>How-to guides<ul> <li>Analysis</li> <li>Find all books</li> <li>Create a new volume</li> </ul> </li> <li>Tutorials</li> <li>Further background<ul> <li>Explanation</li> </ul> </li> <li>API reference</li> <li>Development</li> <li>Changelog</li> </ul>"},{"location":"changelog/","title":"Changelog","text":"<p>Versions follow Semantic Versioning (<code>&lt;major&gt;.&lt;minor&gt;.&lt;patch&gt;</code>).</p> <p>Backward incompatible (breaking) changes will only be introduced in major versions with advance notice in the Deprecations section of releases.</p>"},{"location":"changelog/#bookshelf-v04-2024-10-17","title":"bookshelf v0.4 (2024-10-17)","text":""},{"location":"changelog/#breaking-changes","title":"Breaking Changes","text":"<ul> <li>The <code>bookshelf</code> package has been split into two:</li> <li><code>bookshelf</code> - the core package for consuming content from the bookshelf</li> <li><code>bookshelf-producer</code> - the CLI tool for creating and managing books</li> </ul> <p>This should require no changes for data consumers.   This change makes for a cleaner separation between consuming   and producing datasets.</p> <p>(#65)</p>"},{"location":"changelog/#features","title":"Features","text":"<ul> <li>Added Climate Resource's NDCs dataset to the bookshelf (#56)</li> <li>Add a functions to add long format data and compressed files (#58)</li> <li>Add a functions to get long format data from the book (#59)</li> <li>Added 20240318 version of CAT dataset to the bookshelf (#64)</li> <li>Deploy documentation automatically via the CI (#109)</li> </ul>"},{"location":"changelog/#improvements","title":"Improvements","text":"<ul> <li>When running a notebook, the files were verified through data content hash code rather than file name hash code (#60)</li> <li>Migrate to github (#106)</li> <li>Removed the primap-hist dataset from the repository.</li> </ul> <p>This dataset has been migrated to be a standalone dataset at   climate-resource/bookshelf-primap-hist. (#111) - Moved the <code>bookshelf</code> package to the <code>packages/</code> directory to improve the DX when working with the repository.   This has no user-facing impact. (#112) - Replaced deprecated dependency <code>appdirs</code> with <code>platformdirs</code> (#108) - Pin bookshelf version for producer (#110)</p>"},{"location":"changelog/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>resolve the issue where the upload and download files have rows in a different order. (#63)</li> </ul>"},{"location":"changelog/#improved-documentation","title":"Improved Documentation","text":"<ul> <li>Updated the volume creation documentation (#114)</li> <li>Add example notebooks to docs (#61)</li> <li>Migrated documentation to use mkdocs.   This allows us to write documentation in only MarkDown,   instead of mixing reStructuredText and Markdown. (#66)</li> </ul>"},{"location":"changelog/#trivialinternal-changes","title":"Trivial/Internal Changes","text":"<ul> <li>#107</li> <li>#113</li> <li>#65</li> </ul>"},{"location":"changelog/#bookshelf-v030-2024-01-31","title":"bookshelf v0.3.0 (2024-01-31)","text":""},{"location":"changelog/#features_1","title":"Features","text":"<ul> <li> <ul> <li>Added legacy GDP results from Excel NDC Tool. (#42)</li> </ul> </li> <li>Add an updated version of the World Bank's World Development Indicators (v23). The <code>wdi</code> book has also been   updated to edition 2. (#43)</li> <li> <ul> <li>Added greenhouse gas emissions data from Climate Action Tracker (CAT).</li> </ul> </li> <li>Added historical greenhouse gas emission data and projection data from PBL Netherlands Environmental Assessment Agency.</li> <li>Added estimated energy sector CO2 emissions data from International Energy Agency.</li> </ul> <p>(#45) - Add a function to display the structure of a dataset (#48) - Add data dictionary to schema (#49) - Add data dictionary verification (#50) - Added NGFS3 emissions data. (#53)</p>"},{"location":"changelog/#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>Fix to the schema for datasets to allow no files to be specified (#47)</li> <li>Re-add notebook tests to CI</li> </ul> <p>Updated primap-hist and primap-ssp-downscaled editions to update reflect the renaming of <code>turkey</code> to <code>T\u00fcrkiye</code> (#51)</p>"},{"location":"changelog/#trivialinternal-changes_1","title":"Trivial/Internal Changes","text":"<ul> <li>#55</li> </ul>"},{"location":"changelog/#bookshelf-v024-2023-08-14","title":"bookshelf v0.2.4 (2023-08-14)","text":""},{"location":"changelog/#features_2","title":"Features","text":"<ul> <li>Added the Biennial Reports Common Table Format data reported by Annex-I parties as un-br-ctf.</li> </ul> <p>For now, contains the GHG projections data. (#38)</p>"},{"location":"changelog/#bug-fixes_2","title":"Bug Fixes","text":"<ul> <li>Add CLI entrypoint that was inadvertently missed when migrating to the new copier template. (#39)</li> <li>Fixed the un-br-ctf dataset, now includes a lot more data.</li> </ul> <p>Version 2023-08, edition 1 of the un-br-ctf dataset is to be considered broken, always   use edition 2 instead. (#40)</p>"},{"location":"changelog/#improved-documentation_1","title":"Improved Documentation","text":"<ul> <li>Added documentation about generating and using new versions of Books locally. (#41)</li> </ul>"},{"location":"changelog/#bookshelf-v023-2023-07-28","title":"bookshelf v0.2.3 (2023-07-28)","text":""},{"location":"changelog/#features_3","title":"Features","text":"<ul> <li>Add PRIMAP downscaled SSPs dataset: <code>primap-ssp-downscaled</code> (#34)</li> <li>Migrate to the common Climate Resource copier template</li> </ul> <p>Major changes include adding support for the use of <code>towncrier</code> for managing the changelogs and <code>liccheck</code> for verifying   the compliance of any project dependencies. (#35)</p>"},{"location":"changelog/#improvements_1","title":"Improvements","text":"<ul> <li>Use original region abbreviations in PRIMAP-hist. Bumps <code>primap-hist</code> to edition 4. (#34)</li> <li>Extract SSP marker scenarios in addition to the existing baseline scenarios. Bumps <code>primap-ssp-downscaled</code> to ed.2 (#36)</li> </ul>"},{"location":"changelog/#bug-fixes_3","title":"Bug Fixes","text":"<ul> <li>Convert PRIMAP-hist to units of the form <code>kt X / yr</code> to be consistent. Bumps <code>primap-hist</code> to ed.3 (#32)</li> </ul>"},{"location":"changelog/#v022","title":"v0.2.2","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>(!27) Add sphinx-based documentation</li> <li>(!26) Add <code>force</code> option to the publish CLI command to upload data even if a matching edition already exists</li> <li>(!25) Add primap-hist v2.4.1 and v2.4.2</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>(!29) Move <code>python-dotenv</code> from a development dependency to a core dependency</li> <li>(!23) Fix CEDs unit names for all resources. Bumps <code>ceds</code> to ed.3</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>(!28) Fix file retrieval and publishing on windows</li> </ul>"},{"location":"changelog/#v021","title":"v0.2.1","text":""},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>(!20) Updated <code>DATA_FORMAT_VERSION</code> to <code>v0.2.1</code> in order to handle extra field</li> <li>(!19) Added gwp_context field to primap-hist for easier post processing</li> <li>(!19) Fixed the uploading of new editions</li> </ul>"},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>(!20) Added the option to mark a version as \"private\". This version will not be listed, but can still be loaded if the version is specified.</li> </ul>"},{"location":"changelog/#v020","title":"v0.2.0","text":""},{"location":"changelog/#changed_2","title":"Changed","text":"<ul> <li>(!14) Add sectoral information to CEDS and also support the initial CEDs release as part of Hoesly et al. 2018</li> <li>(!17) Added the concept of editions. Each time the processing changes the edition counter is incremented. The version identifier is reserved for the data source. This results in a breaking change of the data format which has been updated to <code>v0.2.0</code>.</li> <li>(!16)  Updated <code>un-wpp@0.1.2</code> with some fixes to variable naming</li> </ul>"},{"location":"changelog/#v010","title":"v0.1.0","text":""},{"location":"changelog/#changed_3","title":"Changed","text":"<ul> <li>(!12) Update primap-HIST to v0.2.0 to provide resources by region and by country</li> <li>(!11) Remove non-required dependencies from the  requirements</li> <li>(!10) Update issue and MR templates</li> <li>(!7) Renamed <code>LocalBook.metadata</code> to <code>LocalBook.as_datapackage</code></li> <li>(!6) Renamed <code>Bookshelf.save</code> to <code>Bookshelf.publish</code></li> </ul>"},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>(!15) Add <code>un-wpp@v0.1.0</code></li> <li>(!13) Add <code>ceds@0.0.1</code></li> <li>(!9) Add <code>wdi@v0.1.1</code></li> <li>(!8) Add <code>primap-hist@v0.1.0</code></li> <li>(!7) Add <code>Bookshelf.list_versions</code></li> <li>(!6) Add save CLI command</li> <li>(!5) Add CLI tool, <code>bookshelf</code> and CI test suite for notebooks</li> <li>(!4) Add NotebookMetadata schema and an example notebook with documentation</li> <li>(!3) Add ability to upload Books to a remote bookshelf</li> <li>(!2) Add precommit hooks and test coverage to the CI</li> <li>(!1) Add bandit and mypy to the CI</li> <li>Initial project setup</li> </ul>"},{"location":"configuration/","title":"Configuration","text":""},{"location":"configuration/#environment-variables","title":"Environment Variables","text":"<p>Environment variables are used to control some aspects of the model. The default values for these environment variables are generally suitable, but if you require updating these values we recommend the use of a <code>.env</code> file to make the changes easier to reproduce in future.</p>"},{"location":"configuration/#bookshelf_remote","title":"<code>BOOKSHELF_REMOTE</code>","text":"<p>The URL for the remote bookshelf</p> <p>This defaults to a Climate Resource specific URL, but can be used to point to an alternative bookshelf. For example a staging/testing bookshelf for prereleased Books.</p>"},{"location":"configuration/#bookshelf_cache_location","title":"<code>BOOKSHELF_CACHE_LOCATION</code>","text":"<p>Local directory used to cache any Books fetched from a remote bookshelf. This cache can be cleared using the <code>bookshelf clear</code> command.</p>"},{"location":"configuration/#bookshelf_download_cache_location","title":"<code>BOOKSHELF_DOWNLOAD_CACHE_LOCATION</code>","text":"<p>Override the default download location for any raw data downloads</p>"},{"location":"configuration/#bookshelf_notebook_directory","title":"<code>BOOKSHELF_NOTEBOOK_DIRECTORY</code>","text":"<p>Search location for the notebooks used to generate books</p>"},{"location":"development/","title":"Development","text":"<p>Notes for developers. If you want to get involved, please do! We welcome all kinds of contributions, for example:</p> <ul> <li>docs fixes/clarifications</li> <li>bug reports</li> <li>bug fixes</li> <li>feature requests</li> <li>pull requests</li> <li>tutorials</li> </ul>"},{"location":"development/#workflows","title":"Workflows","text":"<p>We don't mind whether you use a branching or forking workflow. However, please only push to your own branches, pushing to other people's branches is often a recipe for disaster, is never required in our experience so is best avoided.</p> <p>Try and keep your merge requests as small as possible (focus on one thing if you can). This makes life much easier for reviewers which allows contributions to be accepted at a faster rate.</p>"},{"location":"development/#installation","title":"Installation","text":"<p>For development, we rely on uv for all our dependency management. To get started, you will need to make sure that <code>uv</code> is installed (instructions here).</p> <p>This project is a <code>uv</code> workspace, which means that it contains more than one Python package. <code>uv</code> commands will by default target the root <code>bookshelf</code> package, but if you wish to target another package you can use the <code>--package</code> flag.</p> <p>For all of work, we use our <code>Makefile</code>. You can read the instructions out and run the commands by hand if you wish, but we generally discourage this because it can be error prone. In order to create your environment, run <code>make virtual-environment</code>.</p> <p>If there are any issues, the messages from the <code>Makefile</code> should guide you through. If not, please raise an issue in the issue tracker.</p>"},{"location":"development/#language","title":"Language","text":"<p>We use British English for our development. We do this for consistency with the broader work context of our lead developers.</p>"},{"location":"development/#versioning","title":"Versioning","text":"<p>This package follows the version format described in PEP440 and Semantic Versioning to describe how the version should change depending on the updates to the code base.</p> <p>Our commit messages are written using written to follow the conventional commits standard which makes it easy to find the commits that matter when traversing through the commit history.</p> <p>Note</p> <p>We don't use the commit messages from conventional commits to automatically generate the changelog and release documentation.</p>"},{"location":"development/#the-notebooks-generating-the-datasets","title":"The notebooks generating the datasets","text":"<p>The top-level directory <code>notebooks</code> contains the notebooks used to produce the <code>Book</code>s. Each  notebook  corresponds with a single <code>Volume</code> (collection of <code>Book</code>s with the same <code>name</code>).</p> <p>Each notebook also has a corresponding <code>.yaml</code> file containing the latest metadata for the <code>Book</code>. See the <code>NotebookMetadata</code> schema(<code>bookshelf.schema.NotebookMetadata</code>) for the expected format of this file.</p>"},{"location":"development/#creating-a-new-volume","title":"Creating a new <code>Volume</code>","text":"<ul> <li>Start by copying <code>example.py</code> and <code>example.yaml</code> and renaming to the name of   the new volume. This provides a simple example to get started.</li> <li>Update <code>{volume}.yaml</code> with the correct metadata</li> <li>Update the fetch and processing steps as needed, adding additional <code>Resource</code>s   to the <code>Book</code> as needed.</li> <li>Run the notebook and check the output</li> <li>TODO Perform the release procedure to upload the built book to the remote   <code>BookShelf</code> <code>bookshelf save {volume}</code></li> </ul>"},{"location":"development/#updating-a-volumes-version","title":"Updating a <code>Volume</code>'s version","text":"<ul> <li>Update the <code>version</code> attribute in the metadata file</li> <li>Modify other metadata attributes as needed</li> <li>Update the data fetching and processing steps in the notebook</li> <li>Run the notebook and check the output</li> <li>TODO Perform the release procedure to upload the built book to the remote   <code>BookShelf</code></li> </ul>"},{"location":"development/#testing-a-notebook-locally","title":"Testing a notebook locally","text":"<p>You can run a notebook with a specified output directory for local testing: <pre><code>uv run bookshelf run --output /path/to/custom/directory &lt;notebook_name&gt;\n</code></pre></p> <p>The generated book can then be used directly from the local directory. Note that the path to the custom directory needs to specify the <code>version</code> of the Book. When loading the Book, you must also specify the version and the edition otherwise it will query the remote bookshelf.</p> <p><pre><code>import bookshelf\n\nshelf = bookshelf.BookShelf(\"/path/to/custom/directory/{version}\")\nedition = 1\n\nnew_book = shelf.load(\"{notebook_name}\", version=\"{version}\", edition=edition)\n</code></pre> When updating an existing Book, remember to increase the version or the edition to make sure you load your newly generated data, not the old data.</p>"},{"location":"development/#releasing","title":"Releasing","text":"<p>Releasing is semi-automated via a CI job. The CI job requires the type of version bump that will be performed to be manually specified. See the poetry docs for the list of available bump rules.</p>"},{"location":"development/#standard-process","title":"Standard process","text":"<p>The steps required are the following:</p> <ol> <li> <p>Bump the version: manually trigger the \"bump\" stage from the latest commit    in main (pipelines are here).    A valid \"bump_rule\" (see https://python-poetry.org/docs/cli/#version)    will need to be specified via the \"BUMP_RULE\" CI    variable (see https://docs.gitlab.com/ee/ci/variables/). This will then    trigger a release, including publication to PyPI.</p> </li> <li> <p>Download the artefacts from the release job. The <code>release_notes.md</code> artefact    will be pre-filled with the list of changes included in this release. You find it    in the release-bundle zip file at    the artefacts section. The    announcements section should be completed manually to highlight any    particularly notable changes or other announcements (or deleted if not    relevant for this release).</p> </li> <li> <p>Once the release notes are filled out, use them to make a    release.</p> </li> <li> <p>That's it, release done, make noise on social media of choice, do whatever    else</p> </li> <li> <p>Enjoy the newly available version</p> </li> </ol>"},{"location":"explanation/","title":"Explanation","text":"<p>This part of the project documentation will focus on an understanding-oriented approach. Here, we will describe the background of the project, as well as reasoning about how it was implemented.</p> <p>Points we will aim to cover:</p> <ul> <li>Context and background on the library</li> <li>Why it was created</li> <li>Help the reader make connections</li> </ul> <p>We will aim to avoid writing instructions or technical descriptions here, they belong elsewhere.</p>"},{"location":"getting_started/","title":"Getting started","text":"<p>There are a few different ways to install and use <code>Bookshelf</code>. We provide these by use case below. As a short summary, if you:</p> <ul> <li>just want to use data stored on the bookshelf,   and nothing else, go to for data consumers</li> <li>want to publish a new book to the bookshelf,   go to for data curators</li> <li>want to develop/modify the bookshelf packages,   go to installation for developers</li> </ul>"},{"location":"getting_started/#by-use-case","title":"By use case","text":""},{"location":"getting_started/#for-data-consumers","title":"For data consumers","text":"<p><code>bookshelf</code> can be installed via pip:</p> <pre><code>pip install bookshelf\n</code></pre> <p>Fetching and using <code>Books</code> requires very little setup in order to start playing with data.</p> <pre><code>&gt;&gt; import bookshelf\n&gt;&gt; shelf = bookshelf.BookShelf()\n# Load the latest version of the MAGICC specific rcmip emissions\n&gt;&gt; book = shelf.load(\"rcmip-emissions\")\nINFO:/home/user/.cache/bookshelf/v0.1.0/rcmip-emissions/volume.json downloaded from https://cr-prod-datasets-bookshelf.s3.us-west-2.amazonaws.com/v0.1.0/rcmip-emissions/volume.json\n# On the first call this will fetch the data from the server and cache locally\n&gt;&gt; book.timeseries(\"magicc\")\nINFO:/home/user/.cache/bookshelf/v0.1.0/rcmip-emissions/v0.0.2/magicc.csv downloaded from https://cr-prod-datasets-bookshelf.s3.us-west-2.amazonaws.com/v0.1.0/rcmip-emissions/v0.0.2/magicc.csv\n&lt;ScmRun (timeseries: 1683, timepoints: 751)&gt;\nTime:\n        Start: 1750-01-01T00:00:00\n        End: 2500-01-01T00:00:00\nMeta:\n                 activity_id mip_era        model region          scenario       unit                    variable\n        0     not_applicable   CMIP5          AIM  World             rcp60   Mt BC/yr                Emissions|BC\n        1     not_applicable   CMIP5          AIM  World             rcp60  Mt CH4/yr               Emissions|CH4\n        2     not_applicable   CMIP5          AIM  World             rcp60   Mt CO/yr                Emissions|CO\n        3     not_applicable   CMIP5          AIM  World             rcp60  Mt CO2/yr               Emissions|CO2\n        4     not_applicable   CMIP5          AIM  World             rcp60  Mt CO2/yr  Emissions|CO2|MAGICC AFOLU\n        ...              ...     ...          ...    ...               ...        ...                         ...\n        1678  not_applicable   CMIP5  unspecified  World  historical-cmip5  Mt NH3/yr               Emissions|NH3\n        1679  not_applicable   CMIP5  unspecified  World  historical-cmip5  Mt NOx/yr               Emissions|NOx\n        1680  not_applicable   CMIP5  unspecified  World  historical-cmip5   Mt OC/yr                Emissions|OC\n        1681  not_applicable   CMIP5  unspecified  World  historical-cmip5  Mt SO2/yr            Emissions|Sulfur\n        1682  not_applicable   CMIP5  unspecified  World  historical-cmip5  Mt VOC/yr               Emissions|VOC\n\n        [1683 rows x 7 columns]\n\n# Subsequent calls use the result from the cache\n&gt;&gt; book.timeseries(\"magicc\")\n</code></pre>"},{"location":"getting_started/#for-data-curators","title":"For data curators","text":"<p>If you wish to build/modify <code>Books</code> some additional dependencies are required. These can be installed using:</p> <pre><code>pip install bookshelf-producer\n</code></pre> <p>Building and deploying datasets is managed via Jupyter notebooks and a small yaml file that contains metadata about the dataset. These notebooks are stored as plain text Python files using the jupytext plugin for Jupyter. See notebooks/example.py for an example dataset.</p> <p>Once the dataset has been developed, it can be deployed to the remote <code>BookShelf</code> so that other users can consume it.</p> <p>The dataset can deployed using the <code>publish</code> CLI as shown below:</p> <pre><code>bookshelf publish my-dataset\n</code></pre> <p>Note</p> <p>Publishing to the remote bookshelf requires valid credentials. Creating or obtaining these credentials is not covered in this documentation.</p>"},{"location":"getting_started/#for-developers","title":"For developers","text":"<p>For development, we rely on uv for all our dependency management. To get started, you will need to make sure that <code>uv</code> is installed (instructions here).</p> <p>This project is a <code>uv</code> workspace, which means that it contains more than one Python package. <code>uv</code> commands will by default target the root <code>bookshelf</code> package, but if you wish to target another package you can use the <code>--package</code> flag.</p> <p>For all of work, we use our <code>Makefile</code>. You can read the instructions out and run the commands by hand if you wish, but we generally discourage this because it can be error prone. In order to create your environment, run <code>make virtual-environment</code>.</p> <p>If there are any issues, the messages from the <code>Makefile</code> should guide you through. If not, please raise an issue in the issue tracker.</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>This part of the project documentation will focus on a learning-oriented approach. You'll learn how to get started with the code in this project.</p> <p>It is currently empty, but we will aim to:</p> <ul> <li>Help newcomers with getting started</li> <li>Teach readers about the library by having them write code</li> <li>Inspire confidence through examples that work for everyone, repeatably</li> <li>Give readers an immediate sense of achievement</li> <li>Show concrete examples without abstractions</li> <li>Provide the minimum necessary explanation</li> <li>Avoid any distraction</li> </ul>"},{"location":"api/NAVIGATION/","title":"NAVIGATION","text":"<ul> <li>bookshelf<ul> <li>book</li> <li>constants</li> <li>dataset_structure</li> <li>errors</li> <li>schema</li> <li>shelf</li> <li>utils</li> </ul> </li> <li>bookshelf_producer<ul> <li>actions</li> <li>cli</li> <li>commands<ul> <li>cmd_publish</li> <li>cmd_run</li> </ul> </li> <li>constants</li> <li>notebook</li> </ul> </li> </ul>"},{"location":"api/bookshelf/","title":"bookshelf","text":"Sub-package Description book A Book represents a single versioned dataset. constants Constants dataset_structure Functionality for interacting with and visualising the structure of a dataset. errors Custom exceptions schema Schema shelf A BookShelf is a collection of Books that can be queried and fetched as needed. utils Bookshelf utilities"},{"location":"api/bookshelf/#bookshelf","title":"<code>bookshelf</code>","text":"<p>A collection of curated climate data sets</p>"},{"location":"api/bookshelf/#bookshelf.BookShelf","title":"<code>BookShelf</code>","text":"<p>A BookShelf stores a number of Books</p> <p>If a Book isn't available locally, it will be queried from the remote bookshelf.</p> <p>Books can be fetched using load by name. Specific versions of a book can be pinned if needed, otherwise the latest version of the book is loaded.</p> Source code in <code>packages/bookshelf/src/bookshelf/shelf.py</code> <pre><code>class BookShelf:\n    \"\"\"\n    A BookShelf stores a number of Books\n\n    If a Book isn't available locally, it will be queried from the remote bookshelf.\n\n    Books can be fetched using [load][bookshelf.BookShelf.load] by name.\n    Specific versions of a book can be pinned if needed,\n    otherwise the latest version of the book is loaded.\n    \"\"\"\n\n    def __init__(\n        self,\n        path: str | pathlib.Path | None = None,\n        remote_bookshelf: str | None = None,\n    ):\n        if path is None:\n            path = create_local_cache(path)\n        self.path = pathlib.Path(path)\n        self.remote_bookshelf = get_remote_bookshelf(remote_bookshelf)\n\n    def load(\n        self,\n        name: str,\n        version: Version | None = None,\n        edition: Edition | None = None,\n        force: bool = False,\n    ) -&gt; LocalBook:\n        \"\"\"\n        Load a book\n\n        If the book's metadata does not exist locally or an unknown version is requested\n        the remote bookshelf is queried, otherwise the local metadata is used.\n\n        Parameters\n        ----------\n        name: str\n            Name of the volume to load\n        version: str\n            Version to load\n\n            If no version is provided, the latest version is returned\n        edition: int\n            Edition of book to load\n\n            If no edition is provided, the latest edition of the selected version is returned\n        force: bool\n            If True, redownload the book metadata\n\n        Raises\n        ------\n        UnknownVersion\n            The requested version is not available for the selected volume\n        UnknownBook\n            An invalid volume is requested\n\n        Returns\n        -------\n        :class:`LocalBook`\n            A book from which the resources can be accessed\n        \"\"\"\n        if version is None or edition is None or force:\n            version, edition = self._resolve_version(name, version, edition)\n        metadata_fragment = LocalBook.relative_path(name, version, edition, \"datapackage.json\")\n        metadata_fname = self.path / metadata_fragment\n        if not metadata_fname.exists():\n            try:\n                url = build_url(\n                    self.remote_bookshelf,\n                    *LocalBook.path_parts(name, version, edition, \"datapackage.json\"),\n                )\n                fetch_file(\n                    url,\n                    local_fname=metadata_fname,\n                    known_hash=None,\n                    force=force,\n                )\n            except requests.exceptions.HTTPError as http_error:\n                raise UnknownVersion(name, version) from http_error\n\n        if not metadata_fname.exists():\n            raise AssertionError()\n        return LocalBook(name, version, edition, local_bookshelf=self.path)\n\n    def is_available(\n        self,\n        name: str,\n        version: Version | None = None,\n        edition: Edition | None = None,\n    ) -&gt; bool:\n        \"\"\"\n        Check if a Book is available from the remote bookshelf\n\n        Parameters\n        ----------\n        name : str\n            Name of the volume to check\n        version : str\n            Version of the volume to check\n\n            If no version is provided, then check if any Book's with a matching name\n            have been uploaded.\n\n        Returns\n        -------\n        bool\n            True if a Book with a matching name and version exists on the remote bookshelf\n        \"\"\"\n        try:\n            self._resolve_version(name, version, edition)\n        except (UnknownBook, UnknownVersion, UnknownEdition):\n            return False\n        return True\n\n    def is_cached(self, name: str, version: Version, edition: Edition) -&gt; bool:\n        \"\"\"\n        Check if a book with a matching name/version is cached on the local bookshelf\n\n        Parameters\n        ----------\n        name : str\n            Name of the volume to check\n        version : str\n            Version of the volume to check\n        edition : int\n            Edition of the volume to check\n\n        Returns\n        -------\n        bool\n            True if a matching book is cached locally\n        \"\"\"\n        try:\n            # Check if the metadata for the book can be successfully read\n            book = LocalBook(name, version, edition, local_bookshelf=self.path)\n            book.metadata()\n        except FileNotFoundError:\n            return False\n        return True\n\n    def _resolve_version(\n        self,\n        name: str,\n        version: Version | None = None,\n        edition: Edition | None = None,\n    ) -&gt; tuple[Version, Edition]:\n        # Update the package metadata\n        try:\n            meta = fetch_volume_meta(name, self.remote_bookshelf, self.path)\n        except requests.exceptions.HTTPError as http_error:\n            raise UnknownBook(f\"No metadata for {name!r}\") from http_error\n\n        if version is None:\n            version = meta.get_latest_version()\n\n        # Verify that the version exists\n        matching_version_books = meta.get_version(version)\n        if not matching_version_books:\n            raise UnknownVersion(name, version)\n\n        # Find edition\n        if edition is None:\n            edition = matching_version_books[-1].edition\n        if edition not in [b.edition for b in matching_version_books]:\n            raise UnknownEdition(name, version, edition)\n        return version, edition\n\n    def list_versions(self, name: str) -&gt; list[str]:\n        \"\"\"\n        Get a list of available versions for a given Book\n\n        Parameters\n        ----------\n        name: str\n            Name of book\n\n        Returns\n        -------\n        list of str\n            List of available versions\n        \"\"\"\n        try:\n            meta = fetch_volume_meta(name, self.remote_bookshelf, self.path)\n        except requests.exceptions.HTTPError as http_error:\n            raise UnknownBook(f\"No metadata for {name!r}\") from http_error\n\n        return [version.version for version in meta.versions if not version.private]\n\n    def list_books(self) -&gt; list[str]:\n        \"\"\"\n        Get a list of book names\n\n        Returns\n        -------\n        list of str\n            List of available books\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/bookshelf/#bookshelf.BookShelf.is_available","title":"<code>is_available(name, version=None, edition=None)</code>","text":"<p>Check if a Book is available from the remote bookshelf</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the volume to check</p> required <code>version</code> <code>str</code> <p>Version of the volume to check</p> <p>If no version is provided, then check if any Book's with a matching name have been uploaded.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if a Book with a matching name and version exists on the remote bookshelf</p> Source code in <code>packages/bookshelf/src/bookshelf/shelf.py</code> <pre><code>def is_available(\n    self,\n    name: str,\n    version: Version | None = None,\n    edition: Edition | None = None,\n) -&gt; bool:\n    \"\"\"\n    Check if a Book is available from the remote bookshelf\n\n    Parameters\n    ----------\n    name : str\n        Name of the volume to check\n    version : str\n        Version of the volume to check\n\n        If no version is provided, then check if any Book's with a matching name\n        have been uploaded.\n\n    Returns\n    -------\n    bool\n        True if a Book with a matching name and version exists on the remote bookshelf\n    \"\"\"\n    try:\n        self._resolve_version(name, version, edition)\n    except (UnknownBook, UnknownVersion, UnknownEdition):\n        return False\n    return True\n</code></pre>"},{"location":"api/bookshelf/#bookshelf.BookShelf.is_cached","title":"<code>is_cached(name, version, edition)</code>","text":"<p>Check if a book with a matching name/version is cached on the local bookshelf</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the volume to check</p> required <code>version</code> <code>str</code> <p>Version of the volume to check</p> required <code>edition</code> <code>int</code> <p>Edition of the volume to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if a matching book is cached locally</p> Source code in <code>packages/bookshelf/src/bookshelf/shelf.py</code> <pre><code>def is_cached(self, name: str, version: Version, edition: Edition) -&gt; bool:\n    \"\"\"\n    Check if a book with a matching name/version is cached on the local bookshelf\n\n    Parameters\n    ----------\n    name : str\n        Name of the volume to check\n    version : str\n        Version of the volume to check\n    edition : int\n        Edition of the volume to check\n\n    Returns\n    -------\n    bool\n        True if a matching book is cached locally\n    \"\"\"\n    try:\n        # Check if the metadata for the book can be successfully read\n        book = LocalBook(name, version, edition, local_bookshelf=self.path)\n        book.metadata()\n    except FileNotFoundError:\n        return False\n    return True\n</code></pre>"},{"location":"api/bookshelf/#bookshelf.BookShelf.list_books","title":"<code>list_books()</code>","text":"<p>Get a list of book names</p> <p>Returns:</p> Type Description <code>list of str</code> <p>List of available books</p> Source code in <code>packages/bookshelf/src/bookshelf/shelf.py</code> <pre><code>def list_books(self) -&gt; list[str]:\n    \"\"\"\n    Get a list of book names\n\n    Returns\n    -------\n    list of str\n        List of available books\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/bookshelf/#bookshelf.BookShelf.list_versions","title":"<code>list_versions(name)</code>","text":"<p>Get a list of available versions for a given Book</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of book</p> required <p>Returns:</p> Type Description <code>list of str</code> <p>List of available versions</p> Source code in <code>packages/bookshelf/src/bookshelf/shelf.py</code> <pre><code>def list_versions(self, name: str) -&gt; list[str]:\n    \"\"\"\n    Get a list of available versions for a given Book\n\n    Parameters\n    ----------\n    name: str\n        Name of book\n\n    Returns\n    -------\n    list of str\n        List of available versions\n    \"\"\"\n    try:\n        meta = fetch_volume_meta(name, self.remote_bookshelf, self.path)\n    except requests.exceptions.HTTPError as http_error:\n        raise UnknownBook(f\"No metadata for {name!r}\") from http_error\n\n    return [version.version for version in meta.versions if not version.private]\n</code></pre>"},{"location":"api/bookshelf/#bookshelf.BookShelf.load","title":"<code>load(name, version=None, edition=None, force=False)</code>","text":"<p>Load a book</p> <p>If the book's metadata does not exist locally or an unknown version is requested the remote bookshelf is queried, otherwise the local metadata is used.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the volume to load</p> required <code>version</code> <code>Version | None</code> <p>Version to load</p> <p>If no version is provided, the latest version is returned</p> <code>None</code> <code>edition</code> <code>Edition | None</code> <p>Edition of book to load</p> <p>If no edition is provided, the latest edition of the selected version is returned</p> <code>None</code> <code>force</code> <code>bool</code> <p>If True, redownload the book metadata</p> <code>False</code> <p>Raises:</p> Type Description <code>UnknownVersion</code> <p>The requested version is not available for the selected volume</p> <code>UnknownBook</code> <p>An invalid volume is requested</p> <p>Returns:</p> Type Description <code>class:`LocalBook`</code> <p>A book from which the resources can be accessed</p> Source code in <code>packages/bookshelf/src/bookshelf/shelf.py</code> <pre><code>def load(\n    self,\n    name: str,\n    version: Version | None = None,\n    edition: Edition | None = None,\n    force: bool = False,\n) -&gt; LocalBook:\n    \"\"\"\n    Load a book\n\n    If the book's metadata does not exist locally or an unknown version is requested\n    the remote bookshelf is queried, otherwise the local metadata is used.\n\n    Parameters\n    ----------\n    name: str\n        Name of the volume to load\n    version: str\n        Version to load\n\n        If no version is provided, the latest version is returned\n    edition: int\n        Edition of book to load\n\n        If no edition is provided, the latest edition of the selected version is returned\n    force: bool\n        If True, redownload the book metadata\n\n    Raises\n    ------\n    UnknownVersion\n        The requested version is not available for the selected volume\n    UnknownBook\n        An invalid volume is requested\n\n    Returns\n    -------\n    :class:`LocalBook`\n        A book from which the resources can be accessed\n    \"\"\"\n    if version is None or edition is None or force:\n        version, edition = self._resolve_version(name, version, edition)\n    metadata_fragment = LocalBook.relative_path(name, version, edition, \"datapackage.json\")\n    metadata_fname = self.path / metadata_fragment\n    if not metadata_fname.exists():\n        try:\n            url = build_url(\n                self.remote_bookshelf,\n                *LocalBook.path_parts(name, version, edition, \"datapackage.json\"),\n            )\n            fetch_file(\n                url,\n                local_fname=metadata_fname,\n                known_hash=None,\n                force=force,\n            )\n        except requests.exceptions.HTTPError as http_error:\n            raise UnknownVersion(name, version) from http_error\n\n    if not metadata_fname.exists():\n        raise AssertionError()\n    return LocalBook(name, version, edition, local_bookshelf=self.path)\n</code></pre>"},{"location":"api/bookshelf/#bookshelf.LocalBook","title":"<code>LocalBook</code>","text":"<p>               Bases: <code>_Book</code></p> <p>A local instance of a Book</p> <p>A Book consists of a metadata file (<code>datapackage.json</code>) and one or more <code>Resource</code> files. For now, these <code>Resource's</code> are only csv files of timeseries in the IAMC format, but this could be extended in future to handle additional data-types. Resources are fetched from the remote bookshelf when first requested and are cached locally for subsequent use.</p> <p>The <code>Book</code> metadata follow the <code>datapackage</code> specification with some additional metadata specific to this project. That means that each <code>Book</code> also doubles as a <code>datapackage</code>. Once released by the <code>Book</code> author, a <code>Book</code> becomes immutable. If <code>Book</code> authors wish to update the metadata or data contained within a <code>Book</code> they must upload a new version of the <code>Book</code>.</p> Source code in <code>packages/bookshelf/src/bookshelf/book.py</code> <pre><code>class LocalBook(_Book):\n    \"\"\"\n    A local instance of a Book\n\n    A Book consists of a metadata file (`datapackage.json`) and one or more `Resource` files.\n    For now, these `Resource's` are only csv files of timeseries in the IAMC format, but\n    this could be extended in future to handle additional data-types. Resources are fetched from\n    the remote bookshelf when first requested and are cached locally for subsequent use.\n\n    The `Book` metadata follow the `datapackage` specification with some additional metadata\n    specific to this project. That means that each `Book` also doubles as a `datapackage`.\n    Once released by the `Book` author, a `Book` becomes immutable. If `Book` authors\n    wish to update the metadata or data contained within a `Book` they must upload a new\n    version of the `Book`.\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        version: str,\n        edition: int = 1,\n        local_bookshelf: str | pathlib.Path | None = None,\n    ):\n        super().__init__(name, version, edition)\n\n        if local_bookshelf is None:\n            local_bookshelf = create_local_cache(local_bookshelf)\n        self.local_bookshelf = pathlib.Path(local_bookshelf)\n        self._metadata: datapackage.Package | None = None\n\n    def hash(self) -&gt; str:\n        \"\"\"\n        Get the hash for the metadata\n\n        This effectively also hashes the data as the metadata contains the hashes of\n        the local Resource files.\n\n        Returns\n        -------\n        str\n            sha256 sum that is unique for the Book\n        \"\"\"\n        return str(pooch.file_hash(self.local_fname(DATAPACKAGE_FILENAME)))\n\n    def local_fname(self, fname: str) -&gt; str:\n        \"\"\"\n        Get the name of a file in the package\n\n        Parameters\n        ----------\n        fname : str\n            Name of the file\n\n        Returns\n        -------\n        :\n            The filename for the file in the local bookshelf\n        \"\"\"\n        return os.path.join(self.local_bookshelf, self.name, self.long_version(), fname)\n\n    def as_datapackage(self) -&gt; datapackage.Package:\n        \"\"\"\n        Datapackage for the current book\n\n        `datapackage` is used for handling the metadata. Modifying\n        the package also modifies the Book.\n\n        Returns\n        -------\n        `datapackage.Package`\n            Metadata about the Book\n        \"\"\"\n        if self._metadata is None:\n            fname = DATAPACKAGE_FILENAME\n\n            local_fname = self.local_fname(fname)\n            with open(local_fname) as file_handle:\n                file_data = json.load(file_handle)\n\n            self._metadata = datapackage.Package(file_data)\n        return self._metadata\n\n    def metadata(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Metadata about the current book\n\n        Returns\n        -------\n        :\n            Metadata about the Book\n        \"\"\"\n        return cast(dict[str, Any], self.as_datapackage().descriptor)\n\n    def files(self) -&gt; list[str]:\n        \"\"\"\n        List of files that are locally available\n\n        Since each Resource is fetched when first read the number of files present may\n        be less than available on the remote bookshelf.\n\n        Returns\n        -------\n        :\n            List of paths of all Book's files, including `datapackage.json` which contains\n            the metadata about the Book.\n        \"\"\"\n        file_list = glob.glob(self.local_fname(\"*\"))\n        return file_list\n\n    def add_timeseries(\n        self, timeseries_name: str, data: scmdata.ScmRun, compressed: bool = True, write_long: bool = True\n    ) -&gt; None:\n        \"\"\"\n        Add two timeseries resource (wide format and long format) to the Book\n\n        Updates the Books metadata\n\n        Parameters\n        ----------\n        timeseries_name : str\n            Name of the resource\n        data : scmdata.ScmRun\n            Timeseries data to add to the Book\n        compressed: bool\n            Whether compressed the file or not\n        write_long: bool\n            Whether to write the long format timeseries data or not\n        \"\"\"\n        if compressed:\n            compression_info = {\"format\": \"csv.gz\", \"compression\": \"gzip\"}\n        else:\n            compression_info = {\"format\": \"csv\", \"compression\": \"infer\"}\n\n        self.write_wide_timeseries(data, timeseries_name, compression_info)\n        if write_long:\n            self.write_long_timeseries(data, timeseries_name, compression_info)\n\n    def write_wide_timeseries(\n        self, data: scmdata.ScmRun, timeseries_name: str, compression_info: dict[str, str]\n    ) -&gt; None:\n        \"\"\"\n        Add the wide format timeseries data to the Book\n\n        Parameters\n        ----------\n        data : scmdata.ScmRun\n            Timeseries data to add to the Book\n        timeseries_name: str\n            Name of the resource\n        compression_info: dict\n            A dictionary about the format of the file and the compression type\n        \"\"\"\n        shape = \"wide\"\n        metadata = self.as_datapackage()\n\n        name = get_resource_key(timeseries_name=timeseries_name, shape=shape)\n        fname = get_resource_filename(\n            book_name=self.name,\n            long_version=self.long_version(),\n            timeseries_name=timeseries_name,\n            shape=shape,\n            file_format=compression_info[\"format\"],\n        )\n\n        timeseries_data = pd.DataFrame(data.timeseries().sort_index())\n\n        timeseries_data.to_csv(  # type: ignore\n            path_or_buf=self.local_fname(fname),\n            compression=compression_info[\"compression\"],\n        )\n        resource_hash = pooch.hashes.file_hash(self.local_fname(fname))\n        content_hash = hashlib.sha256(timeseries_data.to_csv().encode()).hexdigest()\n        metadata.add_resource(\n            {\n                \"name\": name,\n                \"timeseries_name\": timeseries_name,\n                \"shape\": shape,\n                \"format\": compression_info[\"format\"],\n                \"filename\": fname,\n                \"hash\": resource_hash,\n                \"content_hash\": content_hash,\n            }\n        )\n        metadata.save(self.local_fname(DATAPACKAGE_FILENAME))\n\n    def write_long_timeseries(\n        self, data: scmdata.ScmRun, timeseries_name: str, compression_info: dict[str, str]\n    ) -&gt; None:\n        \"\"\"\n        Add the long format timeseries data to the Book\n\n        Parameters\n        ----------\n        data : scmdata.ScmRun\n            Timeseries data to add to the Book\n        timeseries_name: str\n            Name of the resource\n        compression_info: dict\n            A dictionary about the format of the file and the compression type\n        \"\"\"\n\n        def chunked_melt(\n            data: pd.DataFrame, id_vars: list[str], var_name: str, value_name: str\n        ) -&gt; pd.DataFrame:\n            \"\"\"\n            Melt wide format timeseries data to long format\n\n            Efficiently melts large wide-format timeseries data into long format in chunks,\n            addressing performance and memory issues associated with melting large DataFrames.\n\n            Parameters\n            ----------\n            data : pd.DataFrame\n                The wide-format DataFrame to be melted into long format.\n            id_vars : list[str]\n                Column(s) to use as identifier variables. These columns will be\n                preserved during the melt operation.\n            var_name : str\n                Name to assign to the variable column in the melted DataFrame.\n            value_name : str\n                Name to assign to the value column in the melted DataFrame.\n                This name must not match any existing column labels in `data`.\n\n            Returns\n            -------\n            pd.DataFrame\n                The melted DataFrame in long format, combining all chunks.\n            \"\"\"\n            pivot_list = list()\n            chunk_size = 100000\n\n            for i in range(0, len(data), chunk_size):\n                row_pivot = data.iloc[i : i + chunk_size].melt(\n                    id_vars=id_vars, var_name=var_name, value_name=value_name\n                )\n                pivot_list.append(row_pivot)\n\n            melt_df = pd.concat(pivot_list)\n            return melt_df\n\n        shape = \"long\"\n        metadata = self.as_datapackage()\n\n        name = get_resource_key(timeseries_name=timeseries_name, shape=shape)\n        fname = get_resource_filename(\n            book_name=self.name,\n            long_version=self.long_version(),\n            timeseries_name=timeseries_name,\n            shape=shape,\n            file_format=compression_info[\"format\"],\n        )\n\n        var_lst = list(data.meta.columns)\n        sort_lst = [*var_lst, \"year\"]\n        data_df = pd.DataFrame(data.timeseries().sort_index().reset_index())\n        data_melt = chunked_melt(data_df, var_lst, \"year\", \"values\").sort_values(by=sort_lst)\n        data_melt.to_csv(  # type: ignore\n            path_or_buf=self.local_fname(fname),\n            sep=\",\",\n            index=False,\n            header=True,\n            compression=compression_info[\"compression\"],\n        )\n        resource_hash = pooch.hashes.file_hash(self.local_fname(fname))\n        content_hash = hashlib.sha256(data_melt.to_csv().encode()).hexdigest()\n        metadata.add_resource(\n            {\n                \"name\": name,\n                \"timeseries_name\": timeseries_name,\n                \"shape\": shape,\n                \"format\": compression_info[\"format\"],\n                \"filename\": fname,\n                \"hash\": resource_hash,\n                \"content_hash\": content_hash,\n            }\n        )\n        metadata.save(self.local_fname(DATAPACKAGE_FILENAME))\n\n    @classmethod\n    def create_new(cls, name: str, version: Version, edition: Edition = 1, **kwargs: Any) -&gt; \"LocalBook\":\n        \"\"\"\n        Create a new Book for a given name, version and edition\n\n        Returns\n        -------\n        :\n            An instance of a local book\n        \"\"\"\n        book = LocalBook(name, version, edition, **kwargs)\n        book._metadata = datapackage.Package(\n            {\"name\": name, \"version\": version, \"edition\": edition, \"resources\": []}\n        )\n        book._metadata.save(book.local_fname(DATAPACKAGE_FILENAME))\n\n        return book\n\n    @classmethod\n    def create_from_metadata(cls, meta: NotebookMetadata, **kwargs: str) -&gt; \"LocalBook\":\n        \"\"\"\n        Create a new book from a notebook\n\n        Parameters\n        ----------\n        meta : NotebookMetadata\n            Metadata about the book\n\n        kwargs\n            Additional arguments passed to :class:`LocalBook`\n\n        Returns\n        -------\n        :\n            An instance of a local book with the datapackage setup\n        \"\"\"\n        book = LocalBook(meta.name, version=meta.version, edition=meta.edition, **kwargs)\n        book._metadata = datapackage.Package(\n            {\n                \"name\": meta.name,\n                \"version\": meta.version,\n                \"private\": meta.private,\n                \"edition\": meta.edition,\n                \"resources\": [],\n            }\n        )\n        book._metadata.save(book.local_fname(DATAPACKAGE_FILENAME))\n\n        return book\n\n    def timeseries(self, timeseries_name: str) -&gt; scmdata.ScmRun:\n        \"\"\"\n        Get a timeseries resource\n\n        If the data is not available in the local cache, it is downloaded from the\n        remote BookShelf.\n\n        Parameters\n        ----------\n        timeseries_name : str\n            Name of the resource\n\n        Returns\n        -------\n        :\n            Timeseries data\n\n        \"\"\"\n        timeseries_shape = \"wide\"\n        key_name = get_resource_key(timeseries_name=timeseries_name, shape=timeseries_shape)\n        resource: datapackage.Resource = self.as_datapackage().get_resource(key_name)\n        if resource is None:\n            raise ValueError(f\"Unknown timeseries '{key_name}'\")\n        local_fname = self.local_fname(resource.descriptor[\"filename\"])\n        fetch_file(\n            self.url(resource.descriptor.get(\"filename\")),\n            pathlib.Path(local_fname),\n            known_hash=resource.descriptor.get(\"hash\"),\n        )\n\n        return scmdata.ScmRun(local_fname)\n\n    def get_long_format_data(self, timeseries_name: str) -&gt; pd.DataFrame:\n        \"\"\"\n        Get a timeseries resource in long format\n\n        If the data is not available in the local cache, it is downloaded from the\n        remote BookShelf.\n\n        Parameters\n        ----------\n        timeseries_name : str\n            Name of the volume\n\n        Returns\n        -------\n        :\n            Timeseries data\n\n        \"\"\"\n        timeseries_shape = \"long\"\n        key_name = get_resource_key(timeseries_name=timeseries_name, shape=timeseries_shape)\n        resource: datapackage.Resource = self.as_datapackage().get_resource(key_name)\n        if resource is None:\n            raise ValueError(f\"Unknown timeseries '{key_name}'\")\n        local_fname = self.local_fname(resource.descriptor[\"filename\"])\n        fetch_file(\n            self.url(resource.descriptor.get(\"filename\")),\n            pathlib.Path(local_fname),\n            known_hash=resource.descriptor.get(\"hash\"),\n        )\n        return pd.read_csv(local_fname)\n</code></pre>"},{"location":"api/bookshelf/#bookshelf.LocalBook.add_timeseries","title":"<code>add_timeseries(timeseries_name, data, compressed=True, write_long=True)</code>","text":"<p>Add two timeseries resource (wide format and long format) to the Book</p> <p>Updates the Books metadata</p> <p>Parameters:</p> Name Type Description Default <code>timeseries_name</code> <code>str</code> <p>Name of the resource</p> required <code>data</code> <code>ScmRun</code> <p>Timeseries data to add to the Book</p> required <code>compressed</code> <code>bool</code> <p>Whether compressed the file or not</p> <code>True</code> <code>write_long</code> <code>bool</code> <p>Whether to write the long format timeseries data or not</p> <code>True</code> Source code in <code>packages/bookshelf/src/bookshelf/book.py</code> <pre><code>def add_timeseries(\n    self, timeseries_name: str, data: scmdata.ScmRun, compressed: bool = True, write_long: bool = True\n) -&gt; None:\n    \"\"\"\n    Add two timeseries resource (wide format and long format) to the Book\n\n    Updates the Books metadata\n\n    Parameters\n    ----------\n    timeseries_name : str\n        Name of the resource\n    data : scmdata.ScmRun\n        Timeseries data to add to the Book\n    compressed: bool\n        Whether compressed the file or not\n    write_long: bool\n        Whether to write the long format timeseries data or not\n    \"\"\"\n    if compressed:\n        compression_info = {\"format\": \"csv.gz\", \"compression\": \"gzip\"}\n    else:\n        compression_info = {\"format\": \"csv\", \"compression\": \"infer\"}\n\n    self.write_wide_timeseries(data, timeseries_name, compression_info)\n    if write_long:\n        self.write_long_timeseries(data, timeseries_name, compression_info)\n</code></pre>"},{"location":"api/bookshelf/#bookshelf.LocalBook.as_datapackage","title":"<code>as_datapackage()</code>","text":"<p>Datapackage for the current book</p> <p><code>datapackage</code> is used for handling the metadata. Modifying the package also modifies the Book.</p> <p>Returns:</p> Type Description <code>`datapackage.Package`</code> <p>Metadata about the Book</p> Source code in <code>packages/bookshelf/src/bookshelf/book.py</code> <pre><code>def as_datapackage(self) -&gt; datapackage.Package:\n    \"\"\"\n    Datapackage for the current book\n\n    `datapackage` is used for handling the metadata. Modifying\n    the package also modifies the Book.\n\n    Returns\n    -------\n    `datapackage.Package`\n        Metadata about the Book\n    \"\"\"\n    if self._metadata is None:\n        fname = DATAPACKAGE_FILENAME\n\n        local_fname = self.local_fname(fname)\n        with open(local_fname) as file_handle:\n            file_data = json.load(file_handle)\n\n        self._metadata = datapackage.Package(file_data)\n    return self._metadata\n</code></pre>"},{"location":"api/bookshelf/#bookshelf.LocalBook.create_from_metadata","title":"<code>create_from_metadata(meta, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a new book from a notebook</p> <p>Parameters:</p> Name Type Description Default <code>meta</code> <code>NotebookMetadata</code> <p>Metadata about the book</p> required <code>kwargs</code> <code>str</code> <p>Additional arguments passed to :class:<code>LocalBook</code></p> <code>{}</code> <p>Returns:</p> Type Description <code>LocalBook</code> <p>An instance of a local book with the datapackage setup</p> Source code in <code>packages/bookshelf/src/bookshelf/book.py</code> <pre><code>@classmethod\ndef create_from_metadata(cls, meta: NotebookMetadata, **kwargs: str) -&gt; \"LocalBook\":\n    \"\"\"\n    Create a new book from a notebook\n\n    Parameters\n    ----------\n    meta : NotebookMetadata\n        Metadata about the book\n\n    kwargs\n        Additional arguments passed to :class:`LocalBook`\n\n    Returns\n    -------\n    :\n        An instance of a local book with the datapackage setup\n    \"\"\"\n    book = LocalBook(meta.name, version=meta.version, edition=meta.edition, **kwargs)\n    book._metadata = datapackage.Package(\n        {\n            \"name\": meta.name,\n            \"version\": meta.version,\n            \"private\": meta.private,\n            \"edition\": meta.edition,\n            \"resources\": [],\n        }\n    )\n    book._metadata.save(book.local_fname(DATAPACKAGE_FILENAME))\n\n    return book\n</code></pre>"},{"location":"api/bookshelf/#bookshelf.LocalBook.create_new","title":"<code>create_new(name, version, edition=1, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a new Book for a given name, version and edition</p> <p>Returns:</p> Type Description <code>LocalBook</code> <p>An instance of a local book</p> Source code in <code>packages/bookshelf/src/bookshelf/book.py</code> <pre><code>@classmethod\ndef create_new(cls, name: str, version: Version, edition: Edition = 1, **kwargs: Any) -&gt; \"LocalBook\":\n    \"\"\"\n    Create a new Book for a given name, version and edition\n\n    Returns\n    -------\n    :\n        An instance of a local book\n    \"\"\"\n    book = LocalBook(name, version, edition, **kwargs)\n    book._metadata = datapackage.Package(\n        {\"name\": name, \"version\": version, \"edition\": edition, \"resources\": []}\n    )\n    book._metadata.save(book.local_fname(DATAPACKAGE_FILENAME))\n\n    return book\n</code></pre>"},{"location":"api/bookshelf/#bookshelf.LocalBook.files","title":"<code>files()</code>","text":"<p>List of files that are locally available</p> <p>Since each Resource is fetched when first read the number of files present may be less than available on the remote bookshelf.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of paths of all Book's files, including <code>datapackage.json</code> which contains the metadata about the Book.</p> Source code in <code>packages/bookshelf/src/bookshelf/book.py</code> <pre><code>def files(self) -&gt; list[str]:\n    \"\"\"\n    List of files that are locally available\n\n    Since each Resource is fetched when first read the number of files present may\n    be less than available on the remote bookshelf.\n\n    Returns\n    -------\n    :\n        List of paths of all Book's files, including `datapackage.json` which contains\n        the metadata about the Book.\n    \"\"\"\n    file_list = glob.glob(self.local_fname(\"*\"))\n    return file_list\n</code></pre>"},{"location":"api/bookshelf/#bookshelf.LocalBook.get_long_format_data","title":"<code>get_long_format_data(timeseries_name)</code>","text":"<p>Get a timeseries resource in long format</p> <p>If the data is not available in the local cache, it is downloaded from the remote BookShelf.</p> <p>Parameters:</p> Name Type Description Default <code>timeseries_name</code> <code>str</code> <p>Name of the volume</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Timeseries data</p> Source code in <code>packages/bookshelf/src/bookshelf/book.py</code> <pre><code>def get_long_format_data(self, timeseries_name: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Get a timeseries resource in long format\n\n    If the data is not available in the local cache, it is downloaded from the\n    remote BookShelf.\n\n    Parameters\n    ----------\n    timeseries_name : str\n        Name of the volume\n\n    Returns\n    -------\n    :\n        Timeseries data\n\n    \"\"\"\n    timeseries_shape = \"long\"\n    key_name = get_resource_key(timeseries_name=timeseries_name, shape=timeseries_shape)\n    resource: datapackage.Resource = self.as_datapackage().get_resource(key_name)\n    if resource is None:\n        raise ValueError(f\"Unknown timeseries '{key_name}'\")\n    local_fname = self.local_fname(resource.descriptor[\"filename\"])\n    fetch_file(\n        self.url(resource.descriptor.get(\"filename\")),\n        pathlib.Path(local_fname),\n        known_hash=resource.descriptor.get(\"hash\"),\n    )\n    return pd.read_csv(local_fname)\n</code></pre>"},{"location":"api/bookshelf/#bookshelf.LocalBook.hash","title":"<code>hash()</code>","text":"<p>Get the hash for the metadata</p> <p>This effectively also hashes the data as the metadata contains the hashes of the local Resource files.</p> <p>Returns:</p> Type Description <code>str</code> <p>sha256 sum that is unique for the Book</p> Source code in <code>packages/bookshelf/src/bookshelf/book.py</code> <pre><code>def hash(self) -&gt; str:\n    \"\"\"\n    Get the hash for the metadata\n\n    This effectively also hashes the data as the metadata contains the hashes of\n    the local Resource files.\n\n    Returns\n    -------\n    str\n        sha256 sum that is unique for the Book\n    \"\"\"\n    return str(pooch.file_hash(self.local_fname(DATAPACKAGE_FILENAME)))\n</code></pre>"},{"location":"api/bookshelf/#bookshelf.LocalBook.local_fname","title":"<code>local_fname(fname)</code>","text":"<p>Get the name of a file in the package</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>Name of the file</p> required <p>Returns:</p> Type Description <code>str</code> <p>The filename for the file in the local bookshelf</p> Source code in <code>packages/bookshelf/src/bookshelf/book.py</code> <pre><code>def local_fname(self, fname: str) -&gt; str:\n    \"\"\"\n    Get the name of a file in the package\n\n    Parameters\n    ----------\n    fname : str\n        Name of the file\n\n    Returns\n    -------\n    :\n        The filename for the file in the local bookshelf\n    \"\"\"\n    return os.path.join(self.local_bookshelf, self.name, self.long_version(), fname)\n</code></pre>"},{"location":"api/bookshelf/#bookshelf.LocalBook.metadata","title":"<code>metadata()</code>","text":"<p>Metadata about the current book</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Metadata about the Book</p> Source code in <code>packages/bookshelf/src/bookshelf/book.py</code> <pre><code>def metadata(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Metadata about the current book\n\n    Returns\n    -------\n    :\n        Metadata about the Book\n    \"\"\"\n    return cast(dict[str, Any], self.as_datapackage().descriptor)\n</code></pre>"},{"location":"api/bookshelf/#bookshelf.LocalBook.timeseries","title":"<code>timeseries(timeseries_name)</code>","text":"<p>Get a timeseries resource</p> <p>If the data is not available in the local cache, it is downloaded from the remote BookShelf.</p> <p>Parameters:</p> Name Type Description Default <code>timeseries_name</code> <code>str</code> <p>Name of the resource</p> required <p>Returns:</p> Type Description <code>ScmRun</code> <p>Timeseries data</p> Source code in <code>packages/bookshelf/src/bookshelf/book.py</code> <pre><code>def timeseries(self, timeseries_name: str) -&gt; scmdata.ScmRun:\n    \"\"\"\n    Get a timeseries resource\n\n    If the data is not available in the local cache, it is downloaded from the\n    remote BookShelf.\n\n    Parameters\n    ----------\n    timeseries_name : str\n        Name of the resource\n\n    Returns\n    -------\n    :\n        Timeseries data\n\n    \"\"\"\n    timeseries_shape = \"wide\"\n    key_name = get_resource_key(timeseries_name=timeseries_name, shape=timeseries_shape)\n    resource: datapackage.Resource = self.as_datapackage().get_resource(key_name)\n    if resource is None:\n        raise ValueError(f\"Unknown timeseries '{key_name}'\")\n    local_fname = self.local_fname(resource.descriptor[\"filename\"])\n    fetch_file(\n        self.url(resource.descriptor.get(\"filename\")),\n        pathlib.Path(local_fname),\n        known_hash=resource.descriptor.get(\"hash\"),\n    )\n\n    return scmdata.ScmRun(local_fname)\n</code></pre>"},{"location":"api/bookshelf/#bookshelf.LocalBook.write_long_timeseries","title":"<code>write_long_timeseries(data, timeseries_name, compression_info)</code>","text":"<p>Add the long format timeseries data to the Book</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ScmRun</code> <p>Timeseries data to add to the Book</p> required <code>timeseries_name</code> <code>str</code> <p>Name of the resource</p> required <code>compression_info</code> <code>dict[str, str]</code> <p>A dictionary about the format of the file and the compression type</p> required Source code in <code>packages/bookshelf/src/bookshelf/book.py</code> <pre><code>def write_long_timeseries(\n    self, data: scmdata.ScmRun, timeseries_name: str, compression_info: dict[str, str]\n) -&gt; None:\n    \"\"\"\n    Add the long format timeseries data to the Book\n\n    Parameters\n    ----------\n    data : scmdata.ScmRun\n        Timeseries data to add to the Book\n    timeseries_name: str\n        Name of the resource\n    compression_info: dict\n        A dictionary about the format of the file and the compression type\n    \"\"\"\n\n    def chunked_melt(\n        data: pd.DataFrame, id_vars: list[str], var_name: str, value_name: str\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Melt wide format timeseries data to long format\n\n        Efficiently melts large wide-format timeseries data into long format in chunks,\n        addressing performance and memory issues associated with melting large DataFrames.\n\n        Parameters\n        ----------\n        data : pd.DataFrame\n            The wide-format DataFrame to be melted into long format.\n        id_vars : list[str]\n            Column(s) to use as identifier variables. These columns will be\n            preserved during the melt operation.\n        var_name : str\n            Name to assign to the variable column in the melted DataFrame.\n        value_name : str\n            Name to assign to the value column in the melted DataFrame.\n            This name must not match any existing column labels in `data`.\n\n        Returns\n        -------\n        pd.DataFrame\n            The melted DataFrame in long format, combining all chunks.\n        \"\"\"\n        pivot_list = list()\n        chunk_size = 100000\n\n        for i in range(0, len(data), chunk_size):\n            row_pivot = data.iloc[i : i + chunk_size].melt(\n                id_vars=id_vars, var_name=var_name, value_name=value_name\n            )\n            pivot_list.append(row_pivot)\n\n        melt_df = pd.concat(pivot_list)\n        return melt_df\n\n    shape = \"long\"\n    metadata = self.as_datapackage()\n\n    name = get_resource_key(timeseries_name=timeseries_name, shape=shape)\n    fname = get_resource_filename(\n        book_name=self.name,\n        long_version=self.long_version(),\n        timeseries_name=timeseries_name,\n        shape=shape,\n        file_format=compression_info[\"format\"],\n    )\n\n    var_lst = list(data.meta.columns)\n    sort_lst = [*var_lst, \"year\"]\n    data_df = pd.DataFrame(data.timeseries().sort_index().reset_index())\n    data_melt = chunked_melt(data_df, var_lst, \"year\", \"values\").sort_values(by=sort_lst)\n    data_melt.to_csv(  # type: ignore\n        path_or_buf=self.local_fname(fname),\n        sep=\",\",\n        index=False,\n        header=True,\n        compression=compression_info[\"compression\"],\n    )\n    resource_hash = pooch.hashes.file_hash(self.local_fname(fname))\n    content_hash = hashlib.sha256(data_melt.to_csv().encode()).hexdigest()\n    metadata.add_resource(\n        {\n            \"name\": name,\n            \"timeseries_name\": timeseries_name,\n            \"shape\": shape,\n            \"format\": compression_info[\"format\"],\n            \"filename\": fname,\n            \"hash\": resource_hash,\n            \"content_hash\": content_hash,\n        }\n    )\n    metadata.save(self.local_fname(DATAPACKAGE_FILENAME))\n</code></pre>"},{"location":"api/bookshelf/#bookshelf.LocalBook.write_wide_timeseries","title":"<code>write_wide_timeseries(data, timeseries_name, compression_info)</code>","text":"<p>Add the wide format timeseries data to the Book</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ScmRun</code> <p>Timeseries data to add to the Book</p> required <code>timeseries_name</code> <code>str</code> <p>Name of the resource</p> required <code>compression_info</code> <code>dict[str, str]</code> <p>A dictionary about the format of the file and the compression type</p> required Source code in <code>packages/bookshelf/src/bookshelf/book.py</code> <pre><code>def write_wide_timeseries(\n    self, data: scmdata.ScmRun, timeseries_name: str, compression_info: dict[str, str]\n) -&gt; None:\n    \"\"\"\n    Add the wide format timeseries data to the Book\n\n    Parameters\n    ----------\n    data : scmdata.ScmRun\n        Timeseries data to add to the Book\n    timeseries_name: str\n        Name of the resource\n    compression_info: dict\n        A dictionary about the format of the file and the compression type\n    \"\"\"\n    shape = \"wide\"\n    metadata = self.as_datapackage()\n\n    name = get_resource_key(timeseries_name=timeseries_name, shape=shape)\n    fname = get_resource_filename(\n        book_name=self.name,\n        long_version=self.long_version(),\n        timeseries_name=timeseries_name,\n        shape=shape,\n        file_format=compression_info[\"format\"],\n    )\n\n    timeseries_data = pd.DataFrame(data.timeseries().sort_index())\n\n    timeseries_data.to_csv(  # type: ignore\n        path_or_buf=self.local_fname(fname),\n        compression=compression_info[\"compression\"],\n    )\n    resource_hash = pooch.hashes.file_hash(self.local_fname(fname))\n    content_hash = hashlib.sha256(timeseries_data.to_csv().encode()).hexdigest()\n    metadata.add_resource(\n        {\n            \"name\": name,\n            \"timeseries_name\": timeseries_name,\n            \"shape\": shape,\n            \"format\": compression_info[\"format\"],\n            \"filename\": fname,\n            \"hash\": resource_hash,\n            \"content_hash\": content_hash,\n        }\n    )\n    metadata.save(self.local_fname(DATAPACKAGE_FILENAME))\n</code></pre>"},{"location":"api/bookshelf/book/","title":"bookshelf.book","text":""},{"location":"api/bookshelf/book/#bookshelf.book","title":"<code>bookshelf.book</code>","text":"<p>A Book represents a single versioned dataset.</p> <p>A dataset can contain multiple resources each of which are loaded independently.</p>"},{"location":"api/bookshelf/book/#bookshelf.book.LocalBook","title":"<code>LocalBook</code>","text":"<p>               Bases: <code>_Book</code></p> <p>A local instance of a Book</p> <p>A Book consists of a metadata file (<code>datapackage.json</code>) and one or more <code>Resource</code> files. For now, these <code>Resource's</code> are only csv files of timeseries in the IAMC format, but this could be extended in future to handle additional data-types. Resources are fetched from the remote bookshelf when first requested and are cached locally for subsequent use.</p> <p>The <code>Book</code> metadata follow the <code>datapackage</code> specification with some additional metadata specific to this project. That means that each <code>Book</code> also doubles as a <code>datapackage</code>. Once released by the <code>Book</code> author, a <code>Book</code> becomes immutable. If <code>Book</code> authors wish to update the metadata or data contained within a <code>Book</code> they must upload a new version of the <code>Book</code>.</p> Source code in <code>packages/bookshelf/src/bookshelf/book.py</code> <pre><code>class LocalBook(_Book):\n    \"\"\"\n    A local instance of a Book\n\n    A Book consists of a metadata file (`datapackage.json`) and one or more `Resource` files.\n    For now, these `Resource's` are only csv files of timeseries in the IAMC format, but\n    this could be extended in future to handle additional data-types. Resources are fetched from\n    the remote bookshelf when first requested and are cached locally for subsequent use.\n\n    The `Book` metadata follow the `datapackage` specification with some additional metadata\n    specific to this project. That means that each `Book` also doubles as a `datapackage`.\n    Once released by the `Book` author, a `Book` becomes immutable. If `Book` authors\n    wish to update the metadata or data contained within a `Book` they must upload a new\n    version of the `Book`.\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        version: str,\n        edition: int = 1,\n        local_bookshelf: str | pathlib.Path | None = None,\n    ):\n        super().__init__(name, version, edition)\n\n        if local_bookshelf is None:\n            local_bookshelf = create_local_cache(local_bookshelf)\n        self.local_bookshelf = pathlib.Path(local_bookshelf)\n        self._metadata: datapackage.Package | None = None\n\n    def hash(self) -&gt; str:\n        \"\"\"\n        Get the hash for the metadata\n\n        This effectively also hashes the data as the metadata contains the hashes of\n        the local Resource files.\n\n        Returns\n        -------\n        str\n            sha256 sum that is unique for the Book\n        \"\"\"\n        return str(pooch.file_hash(self.local_fname(DATAPACKAGE_FILENAME)))\n\n    def local_fname(self, fname: str) -&gt; str:\n        \"\"\"\n        Get the name of a file in the package\n\n        Parameters\n        ----------\n        fname : str\n            Name of the file\n\n        Returns\n        -------\n        :\n            The filename for the file in the local bookshelf\n        \"\"\"\n        return os.path.join(self.local_bookshelf, self.name, self.long_version(), fname)\n\n    def as_datapackage(self) -&gt; datapackage.Package:\n        \"\"\"\n        Datapackage for the current book\n\n        `datapackage` is used for handling the metadata. Modifying\n        the package also modifies the Book.\n\n        Returns\n        -------\n        `datapackage.Package`\n            Metadata about the Book\n        \"\"\"\n        if self._metadata is None:\n            fname = DATAPACKAGE_FILENAME\n\n            local_fname = self.local_fname(fname)\n            with open(local_fname) as file_handle:\n                file_data = json.load(file_handle)\n\n            self._metadata = datapackage.Package(file_data)\n        return self._metadata\n\n    def metadata(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Metadata about the current book\n\n        Returns\n        -------\n        :\n            Metadata about the Book\n        \"\"\"\n        return cast(dict[str, Any], self.as_datapackage().descriptor)\n\n    def files(self) -&gt; list[str]:\n        \"\"\"\n        List of files that are locally available\n\n        Since each Resource is fetched when first read the number of files present may\n        be less than available on the remote bookshelf.\n\n        Returns\n        -------\n        :\n            List of paths of all Book's files, including `datapackage.json` which contains\n            the metadata about the Book.\n        \"\"\"\n        file_list = glob.glob(self.local_fname(\"*\"))\n        return file_list\n\n    def add_timeseries(\n        self, timeseries_name: str, data: scmdata.ScmRun, compressed: bool = True, write_long: bool = True\n    ) -&gt; None:\n        \"\"\"\n        Add two timeseries resource (wide format and long format) to the Book\n\n        Updates the Books metadata\n\n        Parameters\n        ----------\n        timeseries_name : str\n            Name of the resource\n        data : scmdata.ScmRun\n            Timeseries data to add to the Book\n        compressed: bool\n            Whether compressed the file or not\n        write_long: bool\n            Whether to write the long format timeseries data or not\n        \"\"\"\n        if compressed:\n            compression_info = {\"format\": \"csv.gz\", \"compression\": \"gzip\"}\n        else:\n            compression_info = {\"format\": \"csv\", \"compression\": \"infer\"}\n\n        self.write_wide_timeseries(data, timeseries_name, compression_info)\n        if write_long:\n            self.write_long_timeseries(data, timeseries_name, compression_info)\n\n    def write_wide_timeseries(\n        self, data: scmdata.ScmRun, timeseries_name: str, compression_info: dict[str, str]\n    ) -&gt; None:\n        \"\"\"\n        Add the wide format timeseries data to the Book\n\n        Parameters\n        ----------\n        data : scmdata.ScmRun\n            Timeseries data to add to the Book\n        timeseries_name: str\n            Name of the resource\n        compression_info: dict\n            A dictionary about the format of the file and the compression type\n        \"\"\"\n        shape = \"wide\"\n        metadata = self.as_datapackage()\n\n        name = get_resource_key(timeseries_name=timeseries_name, shape=shape)\n        fname = get_resource_filename(\n            book_name=self.name,\n            long_version=self.long_version(),\n            timeseries_name=timeseries_name,\n            shape=shape,\n            file_format=compression_info[\"format\"],\n        )\n\n        timeseries_data = pd.DataFrame(data.timeseries().sort_index())\n\n        timeseries_data.to_csv(  # type: ignore\n            path_or_buf=self.local_fname(fname),\n            compression=compression_info[\"compression\"],\n        )\n        resource_hash = pooch.hashes.file_hash(self.local_fname(fname))\n        content_hash = hashlib.sha256(timeseries_data.to_csv().encode()).hexdigest()\n        metadata.add_resource(\n            {\n                \"name\": name,\n                \"timeseries_name\": timeseries_name,\n                \"shape\": shape,\n                \"format\": compression_info[\"format\"],\n                \"filename\": fname,\n                \"hash\": resource_hash,\n                \"content_hash\": content_hash,\n            }\n        )\n        metadata.save(self.local_fname(DATAPACKAGE_FILENAME))\n\n    def write_long_timeseries(\n        self, data: scmdata.ScmRun, timeseries_name: str, compression_info: dict[str, str]\n    ) -&gt; None:\n        \"\"\"\n        Add the long format timeseries data to the Book\n\n        Parameters\n        ----------\n        data : scmdata.ScmRun\n            Timeseries data to add to the Book\n        timeseries_name: str\n            Name of the resource\n        compression_info: dict\n            A dictionary about the format of the file and the compression type\n        \"\"\"\n\n        def chunked_melt(\n            data: pd.DataFrame, id_vars: list[str], var_name: str, value_name: str\n        ) -&gt; pd.DataFrame:\n            \"\"\"\n            Melt wide format timeseries data to long format\n\n            Efficiently melts large wide-format timeseries data into long format in chunks,\n            addressing performance and memory issues associated with melting large DataFrames.\n\n            Parameters\n            ----------\n            data : pd.DataFrame\n                The wide-format DataFrame to be melted into long format.\n            id_vars : list[str]\n                Column(s) to use as identifier variables. These columns will be\n                preserved during the melt operation.\n            var_name : str\n                Name to assign to the variable column in the melted DataFrame.\n            value_name : str\n                Name to assign to the value column in the melted DataFrame.\n                This name must not match any existing column labels in `data`.\n\n            Returns\n            -------\n            pd.DataFrame\n                The melted DataFrame in long format, combining all chunks.\n            \"\"\"\n            pivot_list = list()\n            chunk_size = 100000\n\n            for i in range(0, len(data), chunk_size):\n                row_pivot = data.iloc[i : i + chunk_size].melt(\n                    id_vars=id_vars, var_name=var_name, value_name=value_name\n                )\n                pivot_list.append(row_pivot)\n\n            melt_df = pd.concat(pivot_list)\n            return melt_df\n\n        shape = \"long\"\n        metadata = self.as_datapackage()\n\n        name = get_resource_key(timeseries_name=timeseries_name, shape=shape)\n        fname = get_resource_filename(\n            book_name=self.name,\n            long_version=self.long_version(),\n            timeseries_name=timeseries_name,\n            shape=shape,\n            file_format=compression_info[\"format\"],\n        )\n\n        var_lst = list(data.meta.columns)\n        sort_lst = [*var_lst, \"year\"]\n        data_df = pd.DataFrame(data.timeseries().sort_index().reset_index())\n        data_melt = chunked_melt(data_df, var_lst, \"year\", \"values\").sort_values(by=sort_lst)\n        data_melt.to_csv(  # type: ignore\n            path_or_buf=self.local_fname(fname),\n            sep=\",\",\n            index=False,\n            header=True,\n            compression=compression_info[\"compression\"],\n        )\n        resource_hash = pooch.hashes.file_hash(self.local_fname(fname))\n        content_hash = hashlib.sha256(data_melt.to_csv().encode()).hexdigest()\n        metadata.add_resource(\n            {\n                \"name\": name,\n                \"timeseries_name\": timeseries_name,\n                \"shape\": shape,\n                \"format\": compression_info[\"format\"],\n                \"filename\": fname,\n                \"hash\": resource_hash,\n                \"content_hash\": content_hash,\n            }\n        )\n        metadata.save(self.local_fname(DATAPACKAGE_FILENAME))\n\n    @classmethod\n    def create_new(cls, name: str, version: Version, edition: Edition = 1, **kwargs: Any) -&gt; \"LocalBook\":\n        \"\"\"\n        Create a new Book for a given name, version and edition\n\n        Returns\n        -------\n        :\n            An instance of a local book\n        \"\"\"\n        book = LocalBook(name, version, edition, **kwargs)\n        book._metadata = datapackage.Package(\n            {\"name\": name, \"version\": version, \"edition\": edition, \"resources\": []}\n        )\n        book._metadata.save(book.local_fname(DATAPACKAGE_FILENAME))\n\n        return book\n\n    @classmethod\n    def create_from_metadata(cls, meta: NotebookMetadata, **kwargs: str) -&gt; \"LocalBook\":\n        \"\"\"\n        Create a new book from a notebook\n\n        Parameters\n        ----------\n        meta : NotebookMetadata\n            Metadata about the book\n\n        kwargs\n            Additional arguments passed to :class:`LocalBook`\n\n        Returns\n        -------\n        :\n            An instance of a local book with the datapackage setup\n        \"\"\"\n        book = LocalBook(meta.name, version=meta.version, edition=meta.edition, **kwargs)\n        book._metadata = datapackage.Package(\n            {\n                \"name\": meta.name,\n                \"version\": meta.version,\n                \"private\": meta.private,\n                \"edition\": meta.edition,\n                \"resources\": [],\n            }\n        )\n        book._metadata.save(book.local_fname(DATAPACKAGE_FILENAME))\n\n        return book\n\n    def timeseries(self, timeseries_name: str) -&gt; scmdata.ScmRun:\n        \"\"\"\n        Get a timeseries resource\n\n        If the data is not available in the local cache, it is downloaded from the\n        remote BookShelf.\n\n        Parameters\n        ----------\n        timeseries_name : str\n            Name of the resource\n\n        Returns\n        -------\n        :\n            Timeseries data\n\n        \"\"\"\n        timeseries_shape = \"wide\"\n        key_name = get_resource_key(timeseries_name=timeseries_name, shape=timeseries_shape)\n        resource: datapackage.Resource = self.as_datapackage().get_resource(key_name)\n        if resource is None:\n            raise ValueError(f\"Unknown timeseries '{key_name}'\")\n        local_fname = self.local_fname(resource.descriptor[\"filename\"])\n        fetch_file(\n            self.url(resource.descriptor.get(\"filename\")),\n            pathlib.Path(local_fname),\n            known_hash=resource.descriptor.get(\"hash\"),\n        )\n\n        return scmdata.ScmRun(local_fname)\n\n    def get_long_format_data(self, timeseries_name: str) -&gt; pd.DataFrame:\n        \"\"\"\n        Get a timeseries resource in long format\n\n        If the data is not available in the local cache, it is downloaded from the\n        remote BookShelf.\n\n        Parameters\n        ----------\n        timeseries_name : str\n            Name of the volume\n\n        Returns\n        -------\n        :\n            Timeseries data\n\n        \"\"\"\n        timeseries_shape = \"long\"\n        key_name = get_resource_key(timeseries_name=timeseries_name, shape=timeseries_shape)\n        resource: datapackage.Resource = self.as_datapackage().get_resource(key_name)\n        if resource is None:\n            raise ValueError(f\"Unknown timeseries '{key_name}'\")\n        local_fname = self.local_fname(resource.descriptor[\"filename\"])\n        fetch_file(\n            self.url(resource.descriptor.get(\"filename\")),\n            pathlib.Path(local_fname),\n            known_hash=resource.descriptor.get(\"hash\"),\n        )\n        return pd.read_csv(local_fname)\n</code></pre>"},{"location":"api/bookshelf/book/#bookshelf.book.LocalBook.add_timeseries","title":"<code>add_timeseries(timeseries_name, data, compressed=True, write_long=True)</code>","text":"<p>Add two timeseries resource (wide format and long format) to the Book</p> <p>Updates the Books metadata</p> <p>Parameters:</p> Name Type Description Default <code>timeseries_name</code> <code>str</code> <p>Name of the resource</p> required <code>data</code> <code>ScmRun</code> <p>Timeseries data to add to the Book</p> required <code>compressed</code> <code>bool</code> <p>Whether compressed the file or not</p> <code>True</code> <code>write_long</code> <code>bool</code> <p>Whether to write the long format timeseries data or not</p> <code>True</code> Source code in <code>packages/bookshelf/src/bookshelf/book.py</code> <pre><code>def add_timeseries(\n    self, timeseries_name: str, data: scmdata.ScmRun, compressed: bool = True, write_long: bool = True\n) -&gt; None:\n    \"\"\"\n    Add two timeseries resource (wide format and long format) to the Book\n\n    Updates the Books metadata\n\n    Parameters\n    ----------\n    timeseries_name : str\n        Name of the resource\n    data : scmdata.ScmRun\n        Timeseries data to add to the Book\n    compressed: bool\n        Whether compressed the file or not\n    write_long: bool\n        Whether to write the long format timeseries data or not\n    \"\"\"\n    if compressed:\n        compression_info = {\"format\": \"csv.gz\", \"compression\": \"gzip\"}\n    else:\n        compression_info = {\"format\": \"csv\", \"compression\": \"infer\"}\n\n    self.write_wide_timeseries(data, timeseries_name, compression_info)\n    if write_long:\n        self.write_long_timeseries(data, timeseries_name, compression_info)\n</code></pre>"},{"location":"api/bookshelf/book/#bookshelf.book.LocalBook.as_datapackage","title":"<code>as_datapackage()</code>","text":"<p>Datapackage for the current book</p> <p><code>datapackage</code> is used for handling the metadata. Modifying the package also modifies the Book.</p> <p>Returns:</p> Type Description <code>`datapackage.Package`</code> <p>Metadata about the Book</p> Source code in <code>packages/bookshelf/src/bookshelf/book.py</code> <pre><code>def as_datapackage(self) -&gt; datapackage.Package:\n    \"\"\"\n    Datapackage for the current book\n\n    `datapackage` is used for handling the metadata. Modifying\n    the package also modifies the Book.\n\n    Returns\n    -------\n    `datapackage.Package`\n        Metadata about the Book\n    \"\"\"\n    if self._metadata is None:\n        fname = DATAPACKAGE_FILENAME\n\n        local_fname = self.local_fname(fname)\n        with open(local_fname) as file_handle:\n            file_data = json.load(file_handle)\n\n        self._metadata = datapackage.Package(file_data)\n    return self._metadata\n</code></pre>"},{"location":"api/bookshelf/book/#bookshelf.book.LocalBook.create_from_metadata","title":"<code>create_from_metadata(meta, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a new book from a notebook</p> <p>Parameters:</p> Name Type Description Default <code>meta</code> <code>NotebookMetadata</code> <p>Metadata about the book</p> required <code>kwargs</code> <code>str</code> <p>Additional arguments passed to :class:<code>LocalBook</code></p> <code>{}</code> <p>Returns:</p> Type Description <code>LocalBook</code> <p>An instance of a local book with the datapackage setup</p> Source code in <code>packages/bookshelf/src/bookshelf/book.py</code> <pre><code>@classmethod\ndef create_from_metadata(cls, meta: NotebookMetadata, **kwargs: str) -&gt; \"LocalBook\":\n    \"\"\"\n    Create a new book from a notebook\n\n    Parameters\n    ----------\n    meta : NotebookMetadata\n        Metadata about the book\n\n    kwargs\n        Additional arguments passed to :class:`LocalBook`\n\n    Returns\n    -------\n    :\n        An instance of a local book with the datapackage setup\n    \"\"\"\n    book = LocalBook(meta.name, version=meta.version, edition=meta.edition, **kwargs)\n    book._metadata = datapackage.Package(\n        {\n            \"name\": meta.name,\n            \"version\": meta.version,\n            \"private\": meta.private,\n            \"edition\": meta.edition,\n            \"resources\": [],\n        }\n    )\n    book._metadata.save(book.local_fname(DATAPACKAGE_FILENAME))\n\n    return book\n</code></pre>"},{"location":"api/bookshelf/book/#bookshelf.book.LocalBook.create_new","title":"<code>create_new(name, version, edition=1, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a new Book for a given name, version and edition</p> <p>Returns:</p> Type Description <code>LocalBook</code> <p>An instance of a local book</p> Source code in <code>packages/bookshelf/src/bookshelf/book.py</code> <pre><code>@classmethod\ndef create_new(cls, name: str, version: Version, edition: Edition = 1, **kwargs: Any) -&gt; \"LocalBook\":\n    \"\"\"\n    Create a new Book for a given name, version and edition\n\n    Returns\n    -------\n    :\n        An instance of a local book\n    \"\"\"\n    book = LocalBook(name, version, edition, **kwargs)\n    book._metadata = datapackage.Package(\n        {\"name\": name, \"version\": version, \"edition\": edition, \"resources\": []}\n    )\n    book._metadata.save(book.local_fname(DATAPACKAGE_FILENAME))\n\n    return book\n</code></pre>"},{"location":"api/bookshelf/book/#bookshelf.book.LocalBook.files","title":"<code>files()</code>","text":"<p>List of files that are locally available</p> <p>Since each Resource is fetched when first read the number of files present may be less than available on the remote bookshelf.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of paths of all Book's files, including <code>datapackage.json</code> which contains the metadata about the Book.</p> Source code in <code>packages/bookshelf/src/bookshelf/book.py</code> <pre><code>def files(self) -&gt; list[str]:\n    \"\"\"\n    List of files that are locally available\n\n    Since each Resource is fetched when first read the number of files present may\n    be less than available on the remote bookshelf.\n\n    Returns\n    -------\n    :\n        List of paths of all Book's files, including `datapackage.json` which contains\n        the metadata about the Book.\n    \"\"\"\n    file_list = glob.glob(self.local_fname(\"*\"))\n    return file_list\n</code></pre>"},{"location":"api/bookshelf/book/#bookshelf.book.LocalBook.get_long_format_data","title":"<code>get_long_format_data(timeseries_name)</code>","text":"<p>Get a timeseries resource in long format</p> <p>If the data is not available in the local cache, it is downloaded from the remote BookShelf.</p> <p>Parameters:</p> Name Type Description Default <code>timeseries_name</code> <code>str</code> <p>Name of the volume</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Timeseries data</p> Source code in <code>packages/bookshelf/src/bookshelf/book.py</code> <pre><code>def get_long_format_data(self, timeseries_name: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Get a timeseries resource in long format\n\n    If the data is not available in the local cache, it is downloaded from the\n    remote BookShelf.\n\n    Parameters\n    ----------\n    timeseries_name : str\n        Name of the volume\n\n    Returns\n    -------\n    :\n        Timeseries data\n\n    \"\"\"\n    timeseries_shape = \"long\"\n    key_name = get_resource_key(timeseries_name=timeseries_name, shape=timeseries_shape)\n    resource: datapackage.Resource = self.as_datapackage().get_resource(key_name)\n    if resource is None:\n        raise ValueError(f\"Unknown timeseries '{key_name}'\")\n    local_fname = self.local_fname(resource.descriptor[\"filename\"])\n    fetch_file(\n        self.url(resource.descriptor.get(\"filename\")),\n        pathlib.Path(local_fname),\n        known_hash=resource.descriptor.get(\"hash\"),\n    )\n    return pd.read_csv(local_fname)\n</code></pre>"},{"location":"api/bookshelf/book/#bookshelf.book.LocalBook.hash","title":"<code>hash()</code>","text":"<p>Get the hash for the metadata</p> <p>This effectively also hashes the data as the metadata contains the hashes of the local Resource files.</p> <p>Returns:</p> Type Description <code>str</code> <p>sha256 sum that is unique for the Book</p> Source code in <code>packages/bookshelf/src/bookshelf/book.py</code> <pre><code>def hash(self) -&gt; str:\n    \"\"\"\n    Get the hash for the metadata\n\n    This effectively also hashes the data as the metadata contains the hashes of\n    the local Resource files.\n\n    Returns\n    -------\n    str\n        sha256 sum that is unique for the Book\n    \"\"\"\n    return str(pooch.file_hash(self.local_fname(DATAPACKAGE_FILENAME)))\n</code></pre>"},{"location":"api/bookshelf/book/#bookshelf.book.LocalBook.local_fname","title":"<code>local_fname(fname)</code>","text":"<p>Get the name of a file in the package</p> <p>Parameters:</p> Name Type Description Default <code>fname</code> <code>str</code> <p>Name of the file</p> required <p>Returns:</p> Type Description <code>str</code> <p>The filename for the file in the local bookshelf</p> Source code in <code>packages/bookshelf/src/bookshelf/book.py</code> <pre><code>def local_fname(self, fname: str) -&gt; str:\n    \"\"\"\n    Get the name of a file in the package\n\n    Parameters\n    ----------\n    fname : str\n        Name of the file\n\n    Returns\n    -------\n    :\n        The filename for the file in the local bookshelf\n    \"\"\"\n    return os.path.join(self.local_bookshelf, self.name, self.long_version(), fname)\n</code></pre>"},{"location":"api/bookshelf/book/#bookshelf.book.LocalBook.metadata","title":"<code>metadata()</code>","text":"<p>Metadata about the current book</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Metadata about the Book</p> Source code in <code>packages/bookshelf/src/bookshelf/book.py</code> <pre><code>def metadata(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Metadata about the current book\n\n    Returns\n    -------\n    :\n        Metadata about the Book\n    \"\"\"\n    return cast(dict[str, Any], self.as_datapackage().descriptor)\n</code></pre>"},{"location":"api/bookshelf/book/#bookshelf.book.LocalBook.timeseries","title":"<code>timeseries(timeseries_name)</code>","text":"<p>Get a timeseries resource</p> <p>If the data is not available in the local cache, it is downloaded from the remote BookShelf.</p> <p>Parameters:</p> Name Type Description Default <code>timeseries_name</code> <code>str</code> <p>Name of the resource</p> required <p>Returns:</p> Type Description <code>ScmRun</code> <p>Timeseries data</p> Source code in <code>packages/bookshelf/src/bookshelf/book.py</code> <pre><code>def timeseries(self, timeseries_name: str) -&gt; scmdata.ScmRun:\n    \"\"\"\n    Get a timeseries resource\n\n    If the data is not available in the local cache, it is downloaded from the\n    remote BookShelf.\n\n    Parameters\n    ----------\n    timeseries_name : str\n        Name of the resource\n\n    Returns\n    -------\n    :\n        Timeseries data\n\n    \"\"\"\n    timeseries_shape = \"wide\"\n    key_name = get_resource_key(timeseries_name=timeseries_name, shape=timeseries_shape)\n    resource: datapackage.Resource = self.as_datapackage().get_resource(key_name)\n    if resource is None:\n        raise ValueError(f\"Unknown timeseries '{key_name}'\")\n    local_fname = self.local_fname(resource.descriptor[\"filename\"])\n    fetch_file(\n        self.url(resource.descriptor.get(\"filename\")),\n        pathlib.Path(local_fname),\n        known_hash=resource.descriptor.get(\"hash\"),\n    )\n\n    return scmdata.ScmRun(local_fname)\n</code></pre>"},{"location":"api/bookshelf/book/#bookshelf.book.LocalBook.write_long_timeseries","title":"<code>write_long_timeseries(data, timeseries_name, compression_info)</code>","text":"<p>Add the long format timeseries data to the Book</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ScmRun</code> <p>Timeseries data to add to the Book</p> required <code>timeseries_name</code> <code>str</code> <p>Name of the resource</p> required <code>compression_info</code> <code>dict[str, str]</code> <p>A dictionary about the format of the file and the compression type</p> required Source code in <code>packages/bookshelf/src/bookshelf/book.py</code> <pre><code>def write_long_timeseries(\n    self, data: scmdata.ScmRun, timeseries_name: str, compression_info: dict[str, str]\n) -&gt; None:\n    \"\"\"\n    Add the long format timeseries data to the Book\n\n    Parameters\n    ----------\n    data : scmdata.ScmRun\n        Timeseries data to add to the Book\n    timeseries_name: str\n        Name of the resource\n    compression_info: dict\n        A dictionary about the format of the file and the compression type\n    \"\"\"\n\n    def chunked_melt(\n        data: pd.DataFrame, id_vars: list[str], var_name: str, value_name: str\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Melt wide format timeseries data to long format\n\n        Efficiently melts large wide-format timeseries data into long format in chunks,\n        addressing performance and memory issues associated with melting large DataFrames.\n\n        Parameters\n        ----------\n        data : pd.DataFrame\n            The wide-format DataFrame to be melted into long format.\n        id_vars : list[str]\n            Column(s) to use as identifier variables. These columns will be\n            preserved during the melt operation.\n        var_name : str\n            Name to assign to the variable column in the melted DataFrame.\n        value_name : str\n            Name to assign to the value column in the melted DataFrame.\n            This name must not match any existing column labels in `data`.\n\n        Returns\n        -------\n        pd.DataFrame\n            The melted DataFrame in long format, combining all chunks.\n        \"\"\"\n        pivot_list = list()\n        chunk_size = 100000\n\n        for i in range(0, len(data), chunk_size):\n            row_pivot = data.iloc[i : i + chunk_size].melt(\n                id_vars=id_vars, var_name=var_name, value_name=value_name\n            )\n            pivot_list.append(row_pivot)\n\n        melt_df = pd.concat(pivot_list)\n        return melt_df\n\n    shape = \"long\"\n    metadata = self.as_datapackage()\n\n    name = get_resource_key(timeseries_name=timeseries_name, shape=shape)\n    fname = get_resource_filename(\n        book_name=self.name,\n        long_version=self.long_version(),\n        timeseries_name=timeseries_name,\n        shape=shape,\n        file_format=compression_info[\"format\"],\n    )\n\n    var_lst = list(data.meta.columns)\n    sort_lst = [*var_lst, \"year\"]\n    data_df = pd.DataFrame(data.timeseries().sort_index().reset_index())\n    data_melt = chunked_melt(data_df, var_lst, \"year\", \"values\").sort_values(by=sort_lst)\n    data_melt.to_csv(  # type: ignore\n        path_or_buf=self.local_fname(fname),\n        sep=\",\",\n        index=False,\n        header=True,\n        compression=compression_info[\"compression\"],\n    )\n    resource_hash = pooch.hashes.file_hash(self.local_fname(fname))\n    content_hash = hashlib.sha256(data_melt.to_csv().encode()).hexdigest()\n    metadata.add_resource(\n        {\n            \"name\": name,\n            \"timeseries_name\": timeseries_name,\n            \"shape\": shape,\n            \"format\": compression_info[\"format\"],\n            \"filename\": fname,\n            \"hash\": resource_hash,\n            \"content_hash\": content_hash,\n        }\n    )\n    metadata.save(self.local_fname(DATAPACKAGE_FILENAME))\n</code></pre>"},{"location":"api/bookshelf/book/#bookshelf.book.LocalBook.write_wide_timeseries","title":"<code>write_wide_timeseries(data, timeseries_name, compression_info)</code>","text":"<p>Add the wide format timeseries data to the Book</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ScmRun</code> <p>Timeseries data to add to the Book</p> required <code>timeseries_name</code> <code>str</code> <p>Name of the resource</p> required <code>compression_info</code> <code>dict[str, str]</code> <p>A dictionary about the format of the file and the compression type</p> required Source code in <code>packages/bookshelf/src/bookshelf/book.py</code> <pre><code>def write_wide_timeseries(\n    self, data: scmdata.ScmRun, timeseries_name: str, compression_info: dict[str, str]\n) -&gt; None:\n    \"\"\"\n    Add the wide format timeseries data to the Book\n\n    Parameters\n    ----------\n    data : scmdata.ScmRun\n        Timeseries data to add to the Book\n    timeseries_name: str\n        Name of the resource\n    compression_info: dict\n        A dictionary about the format of the file and the compression type\n    \"\"\"\n    shape = \"wide\"\n    metadata = self.as_datapackage()\n\n    name = get_resource_key(timeseries_name=timeseries_name, shape=shape)\n    fname = get_resource_filename(\n        book_name=self.name,\n        long_version=self.long_version(),\n        timeseries_name=timeseries_name,\n        shape=shape,\n        file_format=compression_info[\"format\"],\n    )\n\n    timeseries_data = pd.DataFrame(data.timeseries().sort_index())\n\n    timeseries_data.to_csv(  # type: ignore\n        path_or_buf=self.local_fname(fname),\n        compression=compression_info[\"compression\"],\n    )\n    resource_hash = pooch.hashes.file_hash(self.local_fname(fname))\n    content_hash = hashlib.sha256(timeseries_data.to_csv().encode()).hexdigest()\n    metadata.add_resource(\n        {\n            \"name\": name,\n            \"timeseries_name\": timeseries_name,\n            \"shape\": shape,\n            \"format\": compression_info[\"format\"],\n            \"filename\": fname,\n            \"hash\": resource_hash,\n            \"content_hash\": content_hash,\n        }\n    )\n    metadata.save(self.local_fname(DATAPACKAGE_FILENAME))\n</code></pre>"},{"location":"api/bookshelf/book/#bookshelf.book.get_resource_filename","title":"<code>get_resource_filename(*, book_name, long_version, timeseries_name, shape, file_format)</code>","text":"<p>Generate a resource filename using specified attributes and file format.</p> <p>This function constructs a filename by concatenating given book attributes with underscores and appending the specified file format.</p> <p>Parameters:</p> Name Type Description Default <code>book_name</code> <code>str</code> <p>The name of the book the resource is associated with.</p> required <code>long_version</code> <code>str</code> <p>The long version identifier for the resource.</p> required <code>timeseries_name</code> <code>str</code> <p>The name of the timeseries the resource represents.</p> required <code>shape</code> <code>str</code> <p>The shape of the data (e.g., 'wide', 'long') the resource contains.</p> required <code>file_format</code> <code>str</code> <p>The file format extension (without the period) for the resource file (e.g., 'csv', 'csv.gz').</p> required <p>Returns:</p> Type Description <code>str</code> <p>The constructed filename in the format <code>{book_name}_{long_version}_{timeseries_name}_{shape}.{file_format}</code>.</p> Source code in <code>packages/bookshelf/src/bookshelf/book.py</code> <pre><code>def get_resource_filename(\n    *, book_name: str, long_version: str, timeseries_name: str, shape: str, file_format: str\n) -&gt; str:\n    \"\"\"\n    Generate a resource filename using specified attributes and file format.\n\n    This function constructs a filename by concatenating given book attributes with underscores\n    and appending the specified file format.\n\n    Parameters\n    ----------\n    book_name : str\n        The name of the book the resource is associated with.\n    long_version : str\n        The long version identifier for the resource.\n    timeseries_name : str\n        The name of the timeseries the resource represents.\n    shape : str\n        The shape of the data (e.g., 'wide', 'long') the resource contains.\n    file_format : str\n        The file format extension (without the period) for the resource file (e.g., 'csv', 'csv.gz').\n\n    Returns\n    -------\n    :\n        The constructed filename in the format\n        `{book_name}_{long_version}_{timeseries_name}_{shape}.{file_format}`.\n    \"\"\"\n    filename_tuple = (book_name, long_version, timeseries_name, shape)\n    filename = \"_\".join(filename_tuple)\n    return f\"{filename}.{file_format}\"\n</code></pre>"},{"location":"api/bookshelf/book/#bookshelf.book.get_resource_key","title":"<code>get_resource_key(*, timeseries_name, shape)</code>","text":"<p>Construct a resource key name by concatenating all given arguments with underscores.</p> <p>Take any number of string arguments, concatenate them using an underscore as a separator, and return the resulting string.</p> <p>Parameters:</p> Name Type Description Default <code>timeseries_name</code> <code>str</code> <p>The name of the timeseries the resource represents.</p> required <code>shape</code> <code>str</code> <p>The shape of the data (e.g., 'wide', 'long') the resource contains.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The concatenated key name formed from all the input arguments.</p> Source code in <code>packages/bookshelf/src/bookshelf/book.py</code> <pre><code>def get_resource_key(*, timeseries_name: str, shape: str) -&gt; str:\n    \"\"\"\n    Construct a resource key name by concatenating all given arguments with underscores.\n\n    Take any number of string arguments, concatenate them using an underscore as a separator,\n    and return the resulting string.\n\n    Parameters\n    ----------\n    timeseries_name : str\n        The name of the timeseries the resource represents.\n    shape : str\n        The shape of the data (e.g., 'wide', 'long') the resource contains.\n\n    Returns\n    -------\n    :\n        The concatenated key name formed from all the input arguments.\n    \"\"\"\n    key_name_tuple = (timeseries_name, shape)\n    key_name = \"_\".join(key_name_tuple)\n    return key_name\n</code></pre>"},{"location":"api/bookshelf/constants/","title":"bookshelf.constants","text":""},{"location":"api/bookshelf/constants/#bookshelf.constants","title":"<code>bookshelf.constants</code>","text":"<p>Constants</p>"},{"location":"api/bookshelf/constants/#bookshelf.constants.DATA_FORMAT_VERSION","title":"<code>DATA_FORMAT_VERSION = 'v0.3.2'</code>  <code>module-attribute</code>","text":"<p>Version of the data format</p> <p>This follows the semantic versioning scheme.</p>"},{"location":"api/bookshelf/constants/#bookshelf.constants.DEFAULT_BOOKSHELF","title":"<code>DEFAULT_BOOKSHELF = f'https://cr-prod-datasets-bookshelf.s3.us-west-2.amazonaws.com/{DATA_FORMAT_VERSION}'</code>  <code>module-attribute</code>","text":"<p>Default URL for the remote bookshelf</p>"},{"location":"api/bookshelf/constants/#bookshelf.constants.ENV_PREFIX","title":"<code>ENV_PREFIX = 'BOOKSHELF_'</code>  <code>module-attribute</code>","text":"<p>Prefix for environment variables</p>"},{"location":"api/bookshelf/constants/#bookshelf.constants.PROCESSED_DATA_DIR","title":"<code>PROCESSED_DATA_DIR = os.path.join(ROOT_DIR, 'data', 'processed', DATA_FORMAT_VERSION)</code>  <code>module-attribute</code>","text":"<p>Default directory for storing outputs</p>"},{"location":"api/bookshelf/constants/#bookshelf.constants.ROOT_DIR","title":"<code>ROOT_DIR = os.path.abspath(Path(__file__).parents[4])</code>  <code>module-attribute</code>","text":"<p>Root directory of the repository</p>"},{"location":"api/bookshelf/dataset_structure/","title":"bookshelf.dataset_structure","text":""},{"location":"api/bookshelf/dataset_structure/#bookshelf.dataset_structure","title":"<code>bookshelf.dataset_structure</code>","text":"<p>Functionality for interacting with and visualising the structure of a dataset.</p>"},{"location":"api/bookshelf/dataset_structure/#bookshelf.dataset_structure.VerificationInfo","title":"<code>VerificationInfo</code>","text":"<p>Result from the data verification process.</p> Source code in <code>packages/bookshelf/src/bookshelf/dataset_structure.py</code> <pre><code>@attrs.define\nclass VerificationInfo:\n    \"\"\"\n    Result from the data verification process.\n    \"\"\"\n\n    column_match: bool = attrs.field(init=False, default=True)\n    col_type_match: bool = attrs.field(init=False, default=True)\n    controlled_vocabulary_match: bool = attrs.field(init=False, default=True)\n    non_na_col_match: bool = attrs.field(init=False, default=True)\n\n    def error_message(self) -&gt; str | None:\n        \"\"\"\n        Check if the data dictionary matches the data.\n\n        Returns\n        -------\n        :\n            A string message indicating the result of the verification if it fails.\n            Otherwise None\n        \"\"\"\n        validation = attrs.asdict(self, recurse=False)\n        for attr, value in validation.items():\n            if not value:\n                return f\"Data dictionary does not match the data: {attr} is not true\"\n\n        return None\n</code></pre>"},{"location":"api/bookshelf/dataset_structure/#bookshelf.dataset_structure.VerificationInfo.error_message","title":"<code>error_message()</code>","text":"<p>Check if the data dictionary matches the data.</p> <p>Returns:</p> Type Description <code>str | None</code> <p>A string message indicating the result of the verification if it fails. Otherwise None</p> Source code in <code>packages/bookshelf/src/bookshelf/dataset_structure.py</code> <pre><code>def error_message(self) -&gt; str | None:\n    \"\"\"\n    Check if the data dictionary matches the data.\n\n    Returns\n    -------\n    :\n        A string message indicating the result of the verification if it fails.\n        Otherwise None\n    \"\"\"\n    validation = attrs.asdict(self, recurse=False)\n    for attr, value in validation.items():\n        if not value:\n            return f\"Data dictionary does not match the data: {attr} is not true\"\n\n    return None\n</code></pre>"},{"location":"api/bookshelf/dataset_structure/#bookshelf.dataset_structure.get_dataset_dictionary","title":"<code>get_dataset_dictionary(data)</code>","text":"<p>Extract unique metadata values from an ScmRun object.</p> <p>This function iterates through the columns of the metadata in the ScmRun object and creates a dictionary. Each key in the dictionary corresponds to a column name, and the associated value is a list of unique values in that column.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ScmRun</code> <p>The input ScmRun object from which metadata will be extracted.</p> required <p>Returns:</p> Type Description <code>dict[str, Iterable[str | float | int]]</code> <p>A dictionary with column names as keys and lists of unique values in those columns as values. The values are extracted from the metadata of the ScmRun object.</p> Source code in <code>packages/bookshelf/src/bookshelf/dataset_structure.py</code> <pre><code>def get_dataset_dictionary(data: ScmRun) -&gt; dict[str, Iterable[str | float | int]]:\n    \"\"\"\n    Extract unique metadata values from an ScmRun object.\n\n    This function iterates through the columns of the metadata in the ScmRun object\n    and creates a dictionary. Each key in the dictionary corresponds to a column name,\n    and the associated value is a list of unique values in that column.\n\n    Parameters\n    ----------\n    data\n        The input ScmRun object from which metadata will be extracted.\n\n    Returns\n    -------\n    :\n        A dictionary with column names as keys and lists of unique values in those\n        columns as values. The values are extracted from the metadata of the ScmRun object.\n    \"\"\"\n    data_dict: dict[str, Iterable[str | int | float]] = {}\n    for column in data.meta_attributes:\n        data_dict[column] = data.get_unique_meta(column)\n\n    return data_dict\n</code></pre>"},{"location":"api/bookshelf/dataset_structure/#bookshelf.dataset_structure.print_dataset_structure","title":"<code>print_dataset_structure(data)</code>","text":"<p>Print the structure of a dataset.</p> <p>This function displays a tabular representation of the dataset's dimensions and their unique values. Each row corresponds to a unique value in a dimension, and each column represents a different dimension of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ScmRun</code> <p>The input dataset in the form of an ScmRun object.</p> required <p>Returns:</p> Type Description <code>None</code> <p>This function does not return anything. It prints the dataset structure directly to the console.</p> Source code in <code>packages/bookshelf/src/bookshelf/dataset_structure.py</code> <pre><code>def print_dataset_structure(data: ScmRun) -&gt; None:\n    \"\"\"\n    Print the structure of a dataset.\n\n    This function displays a tabular representation of the dataset's dimensions\n    and their unique values. Each row corresponds to a unique value in a dimension,\n    and each column represents a different dimension of the dataset.\n\n    Parameters\n    ----------\n    data\n        The input dataset in the form of an ScmRun object.\n\n    Returns\n    -------\n    :\n        This function does not return anything. It prints the dataset structure\n        directly to the console.\n    \"\"\"\n    data_dict = get_dataset_dictionary(data)\n\n    # Convert values to strings once\n    k_lst = list(data_dict.keys())\n    v_lst = [list(map(str, values)) for values in data_dict.values()]\n    max_length = max(len(values) for values in v_lst)\n\n    # Calculate width for each column\n    width_lst = [max(len(str(item)) for item in [key, *values]) + 5 for key, values in zip(k_lst, v_lst)]\n\n    # Print header\n    print(\"\".join(f\"{item:{width}}\" for item, width in zip(k_lst, width_lst)))\n\n    # Print header divider\n    print(\n        \"\".join(\n            f\"{item:{width}}\" for item, width in zip([\"-\" * (width - 5) for width in width_lst], width_lst)\n        )\n    )\n\n    # Print each row of values\n    for i in range(max_length):\n        row_values = [values[i] if i &lt; len(values) else \"\" for values in v_lst]\n        print(\"\".join(f\"{item:{width}}\" for item, width in zip(row_values, width_lst)))\n</code></pre>"},{"location":"api/bookshelf/dataset_structure/#bookshelf.dataset_structure.verify_data_dictionary","title":"<code>verify_data_dictionary(data, notebook_config)</code>","text":"<p>Verify the consistency of data against the specifications in a notebook's data dictionary.</p> <p>This function checks various aspects of data integrity, including column matching, data type consistency, adherence to controlled vocabularies, and the presence of required fields without missing (NA) values. It performs these checks by comparing the actual data with the requirements specified in the data dictionary of the notebook configuration.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ScmRun</code> <p>The data set to be verified.</p> required <code>notebook_config</code> <code>NotebookMetadata</code> <p>Configuration of the notebook, including the data dictionary which contains specifications for data validation.</p> required <p>Returns:</p> Type Description <code>VerificationInfo | None</code> <p>An instance of VerificationInfo that contains the results of the data verification.</p> <p>If the data dictionary is empty, the function returns None, indicating that no verification is necessary.</p> Source code in <code>packages/bookshelf/src/bookshelf/dataset_structure.py</code> <pre><code>def verify_data_dictionary(data: ScmRun, notebook_config: NotebookMetadata) -&gt; VerificationInfo | None:\n    \"\"\"\n    Verify the consistency of data against the specifications in a notebook's data dictionary.\n\n    This function checks various aspects of data integrity, including column matching,\n    data type consistency, adherence to controlled vocabularies,\n    and the presence of required fields without missing (NA) values.\n    It performs these checks by comparing the actual data with the requirements\n    specified in the data dictionary of the notebook configuration.\n\n    Parameters\n    ----------\n    data: ScmRun\n        The data set to be verified.\n    notebook_config: NotebookMetadata\n        Configuration of the notebook, including the data dictionary which contains specifications\n        for data validation.\n\n    Returns\n    -------\n    :\n        An instance of VerificationInfo that contains the results of the data verification.\n\n        If the data dictionary is empty, the function returns None,\n        indicating that no verification is necessary.\n    \"\"\"\n    # Return None if the notebook's data dictionary is empty, indicating no verification needed.\n    if len(notebook_config.data_dictionary) == 0:\n        return None\n\n    verification_info = VerificationInfo()\n\n    # Retrieve unique metadata values from the data for comparison with the data dictionary.\n    unique_values = get_dataset_dictionary(data)\n\n    # Iterate through each entry (dimension) in the data dictionary.\n    for variable in notebook_config.data_dictionary:\n        # Check if the required dimension in the data dictionary not in the data.\n        if variable.name not in data.meta.columns:\n            if variable.required_column is True:\n                verification_info.column_match = False\n            else:\n                continue\n        else:\n            # Validate that the data type of the column in the data matches its specified type\n            # in the data dictionary.\n            try:\n                data.meta[variable.name].astype(variable.type)  # type: ignore\n            except ValueError:\n                verification_info.col_type_match = False\n\n            # If a controlled vocabulary (CV) is defined, check if all data values are included in the CV.\n            if variable.controlled_vocabulary is not None:\n                if not set(unique_values[variable.name]).issubset(\n                    set(\n                        [\n                            controlled_vocabulary.value\n                            for controlled_vocabulary in variable.controlled_vocabulary\n                        ]\n                    )\n                ):\n                    verification_info.controlled_vocabulary_match = False\n\n            # For not allowed missing value(NA) dimensions, verify there are NA values in the data.\n            if variable.allowed_NA is False:\n                if data.meta[variable.name].isna().any():\n                    verification_info.non_na_col_match = False\n    return verification_info\n</code></pre>"},{"location":"api/bookshelf/errors/","title":"bookshelf.errors","text":""},{"location":"api/bookshelf/errors/#bookshelf.errors","title":"<code>bookshelf.errors</code>","text":"<p>Custom exceptions</p>"},{"location":"api/bookshelf/errors/#bookshelf.errors.UnknownBook","title":"<code>UnknownBook</code>","text":"<p>               Bases: <code>ValueError</code></p> <p>An unknown book is requested</p> Source code in <code>packages/bookshelf/src/bookshelf/errors.py</code> <pre><code>class UnknownBook(ValueError):\n    \"\"\"\n    An unknown book is requested\n    \"\"\"\n</code></pre>"},{"location":"api/bookshelf/errors/#bookshelf.errors.UnknownEdition","title":"<code>UnknownEdition</code>","text":"<p>               Bases: <code>UnknownVersion</code></p> <p>An unknown edition is requested</p> Source code in <code>packages/bookshelf/src/bookshelf/errors.py</code> <pre><code>class UnknownEdition(UnknownVersion):\n    \"\"\"\n    An unknown edition is requested\n    \"\"\"\n\n    def __init__(self, name: str, version: str, edition: int):\n        super().__init__(name, version)\n        self.edition = edition\n\n    def __str__(self) -&gt; str:\n        return f\"Could not find {self.name}@{self.version} ed.{self.version}\"\n</code></pre>"},{"location":"api/bookshelf/errors/#bookshelf.errors.UnknownVersion","title":"<code>UnknownVersion</code>","text":"<p>               Bases: <code>ValueError</code></p> <p>An unknown version is requested</p> Source code in <code>packages/bookshelf/src/bookshelf/errors.py</code> <pre><code>class UnknownVersion(ValueError):\n    \"\"\"\n    An unknown version is requested\n    \"\"\"\n\n    def __init__(self, name: str, version: str | None):\n        self.name = name\n        self.version = version\n        super().__init__()\n\n    def __str__(self) -&gt; str:\n        return f\"Could not find {self.name}@{self.version}\"\n</code></pre>"},{"location":"api/bookshelf/errors/#bookshelf.errors.UploadError","title":"<code>UploadError</code>","text":"<p>               Bases: <code>ValueError</code></p> <p>Could not upload a book to the remote bookshelf</p> Source code in <code>packages/bookshelf/src/bookshelf/errors.py</code> <pre><code>class UploadError(ValueError):\n    \"\"\"\n    Could not upload a book to the remote bookshelf\n    \"\"\"\n</code></pre>"},{"location":"api/bookshelf/schema/","title":"bookshelf.schema","text":""},{"location":"api/bookshelf/schema/#bookshelf.schema","title":"<code>bookshelf.schema</code>","text":"<p>Schema</p>"},{"location":"api/bookshelf/schema/#bookshelf.schema.BookVersion","title":"<code>BookVersion</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Version information for a book</p> Source code in <code>packages/bookshelf/src/bookshelf/schema.py</code> <pre><code>class BookVersion(BaseModel):\n    \"\"\"\n    Version information for a book\n    \"\"\"\n\n    version: Version\n    edition: Edition\n    url: str\n    hash: str\n    private: bool | None = False\n</code></pre>"},{"location":"api/bookshelf/schema/#bookshelf.schema.ConfigSchema","title":"<code>ConfigSchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for a given Volume (A collection of Books with the same name)</p> <p>A volume can hold multiple versions of the same data</p> Source code in <code>packages/bookshelf/src/bookshelf/schema.py</code> <pre><code>class ConfigSchema(BaseModel):\n    \"\"\"\n    Schema for a given Volume (A collection of Books with the same name)\n\n    A volume can hold multiple versions of the same data\n    \"\"\"\n\n    name: str\n    edition: Edition\n    description: str | None\n    license: str\n    source_file: str\n    metadata: dict[str, Any]  # TODO: type this\n    versions: list[VersionMetadata]\n    data_dictionary: list[Dimension] = Field(default_factory=list)\n</code></pre>"},{"location":"api/bookshelf/schema/#bookshelf.schema.ControlledVocabularyValue","title":"<code>ControlledVocabularyValue</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A value in a controlled vocabulary</p> Source code in <code>packages/bookshelf/src/bookshelf/schema.py</code> <pre><code>class ControlledVocabularyValue(BaseModel):\n    \"\"\"\n    A value in a controlled vocabulary\n    \"\"\"\n\n    value: str\n    \"\"\"\n    Value of the controlled vocabulary\n    \"\"\"\n    description: str\n    \"\"\"\n    Description of the controlled vocabulary value\n    \"\"\"\n</code></pre>"},{"location":"api/bookshelf/schema/#bookshelf.schema.ControlledVocabularyValue.description","title":"<code>description: str</code>  <code>instance-attribute</code>","text":"<p>Description of the controlled vocabulary value</p>"},{"location":"api/bookshelf/schema/#bookshelf.schema.ControlledVocabularyValue.value","title":"<code>value: str</code>  <code>instance-attribute</code>","text":"<p>Value of the controlled vocabulary</p>"},{"location":"api/bookshelf/schema/#bookshelf.schema.DatasetMetadata","title":"<code>DatasetMetadata</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Metadata about a dataset</p> <p>A dataset may consist of multiple files (:class:<code>FileDownloadInfo</code>)</p> Source code in <code>packages/bookshelf/src/bookshelf/schema.py</code> <pre><code>class DatasetMetadata(BaseModel):\n    \"\"\"\n    Metadata about a dataset\n\n    A dataset may consist of multiple files (:class:`FileDownloadInfo`)\n    \"\"\"\n\n    url: str | None\n    doi: str | None\n    files: list[FileDownloadInfo] = Field(default_factory=list)\n    author: str\n</code></pre>"},{"location":"api/bookshelf/schema/#bookshelf.schema.Dimension","title":"<code>Dimension</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Dimension information</p> <p>A dimension describes a feature of the metadata (expressed as a column in the index of a ScmRun).</p> <p>This dimension can optionally be controlled by a controlled vocabulary which limits the possible values.</p> Source code in <code>packages/bookshelf/src/bookshelf/schema.py</code> <pre><code>class Dimension(BaseModel):\n    \"\"\"\n    Dimension information\n\n    A dimension describes a feature of the metadata\n    (expressed as a column in the index of a [ScmRun][scmdata.run.ScmRun]).\n\n    This dimension can optionally be controlled by a controlled vocabulary\n    which limits the possible values.\n    \"\"\"\n\n    name: str\n    \"\"\"\n    Name of the metadata dimension\n\n    This is the same as the column in [ScmRun][scmdata.run.ScmRun]\n    \"\"\"\n    description: str\n    \"\"\"\n    Description of the metadata dimension\n    \"\"\"\n    type: str\n    \"\"\"\n    Type of the values in the metadata dimension\n\n    This is not currently verified\n    \"\"\"\n    required_column: bool = Field(default=True)\n    \"\"\"\n    Indication about whether this metadata dimension is compulsory in all the dataset in the book\n    \"\"\"\n    allowed_NA: bool\n    \"\"\"\n    Indication about whether all values in this metadata dimension must be non-empty\n    \"\"\"\n    controlled_vocabulary: list[ControlledVocabularyValue] | None = None\n    \"\"\"\n    List of possible controlled vocabulary of this metadata dimension\n    \"\"\"\n</code></pre>"},{"location":"api/bookshelf/schema/#bookshelf.schema.Dimension.allowed_NA","title":"<code>allowed_NA: bool</code>  <code>instance-attribute</code>","text":"<p>Indication about whether all values in this metadata dimension must be non-empty</p>"},{"location":"api/bookshelf/schema/#bookshelf.schema.Dimension.controlled_vocabulary","title":"<code>controlled_vocabulary: list[ControlledVocabularyValue] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>List of possible controlled vocabulary of this metadata dimension</p>"},{"location":"api/bookshelf/schema/#bookshelf.schema.Dimension.description","title":"<code>description: str</code>  <code>instance-attribute</code>","text":"<p>Description of the metadata dimension</p>"},{"location":"api/bookshelf/schema/#bookshelf.schema.Dimension.name","title":"<code>name: str</code>  <code>instance-attribute</code>","text":"<p>Name of the metadata dimension</p> <p>This is the same as the column in ScmRun</p>"},{"location":"api/bookshelf/schema/#bookshelf.schema.Dimension.required_column","title":"<code>required_column: bool = Field(default=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Indication about whether this metadata dimension is compulsory in all the dataset in the book</p>"},{"location":"api/bookshelf/schema/#bookshelf.schema.Dimension.type","title":"<code>type: str</code>  <code>instance-attribute</code>","text":"<p>Type of the values in the metadata dimension</p> <p>This is not currently verified</p>"},{"location":"api/bookshelf/schema/#bookshelf.schema.FileDownloadInfo","title":"<code>FileDownloadInfo</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A File to be downloaded as part of a dataset</p> Source code in <code>packages/bookshelf/src/bookshelf/schema.py</code> <pre><code>class FileDownloadInfo(BaseModel):\n    \"\"\"\n    A File to be downloaded as part of a dataset\n    \"\"\"\n\n    url: str\n    \"\"\"\n    URL of the file to be downloaded\n\n    This can be any URLs supported by [pooch.retrieve][],\n    or a local file if the prefix \"file://\" is used.\n\n    With local files,\n    the filename must be a path relative to the notebooks directory.\n    For example, `file://gdp-ndc-tool/13Mar2023a_CR_gdp_results.csv`.\n    \"\"\"\n    hash: str\n    \"\"\"\n    Hash of the file to be downloaded\n\n    Used to verify the consistency of the downloaded file\n    \"\"\"\n</code></pre>"},{"location":"api/bookshelf/schema/#bookshelf.schema.FileDownloadInfo.hash","title":"<code>hash: str</code>  <code>instance-attribute</code>","text":"<p>Hash of the file to be downloaded</p> <p>Used to verify the consistency of the downloaded file</p>"},{"location":"api/bookshelf/schema/#bookshelf.schema.FileDownloadInfo.url","title":"<code>url: str</code>  <code>instance-attribute</code>","text":"<p>URL of the file to be downloaded</p> <p>This can be any URLs supported by pooch.retrieve, or a local file if the prefix \"file://\" is used.</p> <p>With local files, the filename must be a path relative to the notebooks directory. For example, <code>file://gdp-ndc-tool/13Mar2023a_CR_gdp_results.csv</code>.</p>"},{"location":"api/bookshelf/schema/#bookshelf.schema.NotebookMetadata","title":"<code>NotebookMetadata</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for a running a notebook</p> <p>This represents the metadata for a selected version of a volume</p> Source code in <code>packages/bookshelf/src/bookshelf/schema.py</code> <pre><code>class NotebookMetadata(BaseModel):\n    \"\"\"\n    Schema for a running a notebook\n\n    This represents the metadata for a selected version of a volume\n    \"\"\"\n\n    name: str\n    version: Version\n    edition: Edition\n    description: str | None\n    license: str\n    source_file: str\n    private: bool\n    metadata: dict[str, Any]  # TODO: type this\n    dataset: DatasetMetadata\n    data_dictionary: list[Dimension] = Field(default_factory=list)\n\n    def long_name(self) -&gt; str:\n        \"\"\"\n        Long name of the book\n\n        Includes name and long version\n\n        Returns\n        -------\n        str\n            Long identifier for a book\n        \"\"\"\n        return f\"{self.name}@{self.long_version()}\"\n\n    def long_version(self) -&gt; str:\n        \"\"\"\n        Long version identifier\n\n        Of the form `{version}_e{edition}` e.g. \"v1.0.1_e002\".\n\n        Returns\n        -------\n        str\n            Version identification string\n        \"\"\"\n        return f\"{self.version}_e{self.edition:03}\"\n\n    def download_file(self, idx: int = 0) -&gt; str:\n        \"\"\"\n        Download a dataset file\n\n        Uses `pooch` to manage the downloading, verification and caching of data file.\n        The first call will trigger a download and subsequent calls may use the cached\n        file if the previous download succeeded.\n\n        Parameters\n        ----------\n        idx\n            Index of the file to download (0-based).\n\n            Defaults to the first file if no value is provided\n\n        Returns\n        -------\n        str\n            Filename of the locally downloaded file\n        \"\"\"\n        cache_location = get_env_var(\"DOWNLOAD_CACHE_LOCATION\", raise_on_missing=False, default=None)\n\n        try:\n            file_info = self.dataset.files[idx]\n        except IndexError as e:\n            raise ValueError(\"Requested index does not exist\") from e\n\n        file_hash: str | None = file_info.hash\n        if not file_hash:\n            # replace an empty string with None\n            file_hash = None\n\n        if file_info.url.startswith(\"file://\"):\n            return os.path.join(get_notebook_directory(), file_info.url[7:])\n\n        res: str = pooch.retrieve(\n            file_info.url,\n            known_hash=file_hash,\n            path=cache_location,\n        )\n        return res\n</code></pre>"},{"location":"api/bookshelf/schema/#bookshelf.schema.NotebookMetadata.download_file","title":"<code>download_file(idx=0)</code>","text":"<p>Download a dataset file</p> <p>Uses <code>pooch</code> to manage the downloading, verification and caching of data file. The first call will trigger a download and subsequent calls may use the cached file if the previous download succeeded.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the file to download (0-based).</p> <p>Defaults to the first file if no value is provided</p> <code>0</code> <p>Returns:</p> Type Description <code>str</code> <p>Filename of the locally downloaded file</p> Source code in <code>packages/bookshelf/src/bookshelf/schema.py</code> <pre><code>def download_file(self, idx: int = 0) -&gt; str:\n    \"\"\"\n    Download a dataset file\n\n    Uses `pooch` to manage the downloading, verification and caching of data file.\n    The first call will trigger a download and subsequent calls may use the cached\n    file if the previous download succeeded.\n\n    Parameters\n    ----------\n    idx\n        Index of the file to download (0-based).\n\n        Defaults to the first file if no value is provided\n\n    Returns\n    -------\n    str\n        Filename of the locally downloaded file\n    \"\"\"\n    cache_location = get_env_var(\"DOWNLOAD_CACHE_LOCATION\", raise_on_missing=False, default=None)\n\n    try:\n        file_info = self.dataset.files[idx]\n    except IndexError as e:\n        raise ValueError(\"Requested index does not exist\") from e\n\n    file_hash: str | None = file_info.hash\n    if not file_hash:\n        # replace an empty string with None\n        file_hash = None\n\n    if file_info.url.startswith(\"file://\"):\n        return os.path.join(get_notebook_directory(), file_info.url[7:])\n\n    res: str = pooch.retrieve(\n        file_info.url,\n        known_hash=file_hash,\n        path=cache_location,\n    )\n    return res\n</code></pre>"},{"location":"api/bookshelf/schema/#bookshelf.schema.NotebookMetadata.long_name","title":"<code>long_name()</code>","text":"<p>Long name of the book</p> <p>Includes name and long version</p> <p>Returns:</p> Type Description <code>str</code> <p>Long identifier for a book</p> Source code in <code>packages/bookshelf/src/bookshelf/schema.py</code> <pre><code>def long_name(self) -&gt; str:\n    \"\"\"\n    Long name of the book\n\n    Includes name and long version\n\n    Returns\n    -------\n    str\n        Long identifier for a book\n    \"\"\"\n    return f\"{self.name}@{self.long_version()}\"\n</code></pre>"},{"location":"api/bookshelf/schema/#bookshelf.schema.NotebookMetadata.long_version","title":"<code>long_version()</code>","text":"<p>Long version identifier</p> <p>Of the form <code>{version}_e{edition}</code> e.g. \"v1.0.1_e002\".</p> <p>Returns:</p> Type Description <code>str</code> <p>Version identification string</p> Source code in <code>packages/bookshelf/src/bookshelf/schema.py</code> <pre><code>def long_version(self) -&gt; str:\n    \"\"\"\n    Long version identifier\n\n    Of the form `{version}_e{edition}` e.g. \"v1.0.1_e002\".\n\n    Returns\n    -------\n    str\n        Version identification string\n    \"\"\"\n    return f\"{self.version}_e{self.edition:03}\"\n</code></pre>"},{"location":"api/bookshelf/schema/#bookshelf.schema.VersionMetadata","title":"<code>VersionMetadata</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Metadata about a single version of a book</p> Source code in <code>packages/bookshelf/src/bookshelf/schema.py</code> <pre><code>class VersionMetadata(BaseModel):\n    \"\"\"\n    Metadata about a single version of a book\n    \"\"\"\n\n    version: Version\n    dataset: DatasetMetadata\n    private: bool | None = Field(default=False)\n</code></pre>"},{"location":"api/bookshelf/schema/#bookshelf.schema.VolumeMeta","title":"<code>VolumeMeta</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Schema for a given Volume (A collection of Books with the same name)</p> Source code in <code>packages/bookshelf/src/bookshelf/schema.py</code> <pre><code>class VolumeMeta(BaseModel):\n    \"\"\"\n    Schema for a given Volume (A collection of Books with the same name)\n    \"\"\"\n\n    name: str\n    license: str  # A change in license will require a new volume\n    versions: list[BookVersion]\n\n    def get_latest_version(self) -&gt; Version:\n        \"\"\"\n        Get the latest version for a volume\n\n        Returns\n        -------\n        :\n            String containing the latest version of a given volume\n        \"\"\"\n        ordered_versions = sorted([v.version for v in self.versions if not v.private])\n        if not ordered_versions:\n            raise ValueError(\"No published volumes\")\n\n        return ordered_versions[-1]\n\n    def get_version(self, version: Version) -&gt; list[BookVersion]:\n        \"\"\"\n        Get a set of books for a given version\n\n        Returns\n        -------\n        :\n            List of matching books sorted by edition\n        \"\"\"\n        matching_versions = []\n\n        for version_meta in self.versions:\n            if version_meta.version == version:\n                matching_versions.append(version_meta)\n\n        matching_versions = sorted(matching_versions, key=lambda v: v.edition)\n        return matching_versions\n</code></pre>"},{"location":"api/bookshelf/schema/#bookshelf.schema.VolumeMeta.get_latest_version","title":"<code>get_latest_version()</code>","text":"<p>Get the latest version for a volume</p> <p>Returns:</p> Type Description <code>Version</code> <p>String containing the latest version of a given volume</p> Source code in <code>packages/bookshelf/src/bookshelf/schema.py</code> <pre><code>def get_latest_version(self) -&gt; Version:\n    \"\"\"\n    Get the latest version for a volume\n\n    Returns\n    -------\n    :\n        String containing the latest version of a given volume\n    \"\"\"\n    ordered_versions = sorted([v.version for v in self.versions if not v.private])\n    if not ordered_versions:\n        raise ValueError(\"No published volumes\")\n\n    return ordered_versions[-1]\n</code></pre>"},{"location":"api/bookshelf/schema/#bookshelf.schema.VolumeMeta.get_version","title":"<code>get_version(version)</code>","text":"<p>Get a set of books for a given version</p> <p>Returns:</p> Type Description <code>list[BookVersion]</code> <p>List of matching books sorted by edition</p> Source code in <code>packages/bookshelf/src/bookshelf/schema.py</code> <pre><code>def get_version(self, version: Version) -&gt; list[BookVersion]:\n    \"\"\"\n    Get a set of books for a given version\n\n    Returns\n    -------\n    :\n        List of matching books sorted by edition\n    \"\"\"\n    matching_versions = []\n\n    for version_meta in self.versions:\n        if version_meta.version == version:\n            matching_versions.append(version_meta)\n\n    matching_versions = sorted(matching_versions, key=lambda v: v.edition)\n    return matching_versions\n</code></pre>"},{"location":"api/bookshelf/shelf/","title":"bookshelf.shelf","text":""},{"location":"api/bookshelf/shelf/#bookshelf.shelf","title":"<code>bookshelf.shelf</code>","text":"<p>A BookShelf is a collection of Books that can be queried and fetched as needed.</p>"},{"location":"api/bookshelf/shelf/#bookshelf.shelf.BookShelf","title":"<code>BookShelf</code>","text":"<p>A BookShelf stores a number of Books</p> <p>If a Book isn't available locally, it will be queried from the remote bookshelf.</p> <p>Books can be fetched using load by name. Specific versions of a book can be pinned if needed, otherwise the latest version of the book is loaded.</p> Source code in <code>packages/bookshelf/src/bookshelf/shelf.py</code> <pre><code>class BookShelf:\n    \"\"\"\n    A BookShelf stores a number of Books\n\n    If a Book isn't available locally, it will be queried from the remote bookshelf.\n\n    Books can be fetched using [load][bookshelf.BookShelf.load] by name.\n    Specific versions of a book can be pinned if needed,\n    otherwise the latest version of the book is loaded.\n    \"\"\"\n\n    def __init__(\n        self,\n        path: str | pathlib.Path | None = None,\n        remote_bookshelf: str | None = None,\n    ):\n        if path is None:\n            path = create_local_cache(path)\n        self.path = pathlib.Path(path)\n        self.remote_bookshelf = get_remote_bookshelf(remote_bookshelf)\n\n    def load(\n        self,\n        name: str,\n        version: Version | None = None,\n        edition: Edition | None = None,\n        force: bool = False,\n    ) -&gt; LocalBook:\n        \"\"\"\n        Load a book\n\n        If the book's metadata does not exist locally or an unknown version is requested\n        the remote bookshelf is queried, otherwise the local metadata is used.\n\n        Parameters\n        ----------\n        name: str\n            Name of the volume to load\n        version: str\n            Version to load\n\n            If no version is provided, the latest version is returned\n        edition: int\n            Edition of book to load\n\n            If no edition is provided, the latest edition of the selected version is returned\n        force: bool\n            If True, redownload the book metadata\n\n        Raises\n        ------\n        UnknownVersion\n            The requested version is not available for the selected volume\n        UnknownBook\n            An invalid volume is requested\n\n        Returns\n        -------\n        :class:`LocalBook`\n            A book from which the resources can be accessed\n        \"\"\"\n        if version is None or edition is None or force:\n            version, edition = self._resolve_version(name, version, edition)\n        metadata_fragment = LocalBook.relative_path(name, version, edition, \"datapackage.json\")\n        metadata_fname = self.path / metadata_fragment\n        if not metadata_fname.exists():\n            try:\n                url = build_url(\n                    self.remote_bookshelf,\n                    *LocalBook.path_parts(name, version, edition, \"datapackage.json\"),\n                )\n                fetch_file(\n                    url,\n                    local_fname=metadata_fname,\n                    known_hash=None,\n                    force=force,\n                )\n            except requests.exceptions.HTTPError as http_error:\n                raise UnknownVersion(name, version) from http_error\n\n        if not metadata_fname.exists():\n            raise AssertionError()\n        return LocalBook(name, version, edition, local_bookshelf=self.path)\n\n    def is_available(\n        self,\n        name: str,\n        version: Version | None = None,\n        edition: Edition | None = None,\n    ) -&gt; bool:\n        \"\"\"\n        Check if a Book is available from the remote bookshelf\n\n        Parameters\n        ----------\n        name : str\n            Name of the volume to check\n        version : str\n            Version of the volume to check\n\n            If no version is provided, then check if any Book's with a matching name\n            have been uploaded.\n\n        Returns\n        -------\n        bool\n            True if a Book with a matching name and version exists on the remote bookshelf\n        \"\"\"\n        try:\n            self._resolve_version(name, version, edition)\n        except (UnknownBook, UnknownVersion, UnknownEdition):\n            return False\n        return True\n\n    def is_cached(self, name: str, version: Version, edition: Edition) -&gt; bool:\n        \"\"\"\n        Check if a book with a matching name/version is cached on the local bookshelf\n\n        Parameters\n        ----------\n        name : str\n            Name of the volume to check\n        version : str\n            Version of the volume to check\n        edition : int\n            Edition of the volume to check\n\n        Returns\n        -------\n        bool\n            True if a matching book is cached locally\n        \"\"\"\n        try:\n            # Check if the metadata for the book can be successfully read\n            book = LocalBook(name, version, edition, local_bookshelf=self.path)\n            book.metadata()\n        except FileNotFoundError:\n            return False\n        return True\n\n    def _resolve_version(\n        self,\n        name: str,\n        version: Version | None = None,\n        edition: Edition | None = None,\n    ) -&gt; tuple[Version, Edition]:\n        # Update the package metadata\n        try:\n            meta = fetch_volume_meta(name, self.remote_bookshelf, self.path)\n        except requests.exceptions.HTTPError as http_error:\n            raise UnknownBook(f\"No metadata for {name!r}\") from http_error\n\n        if version is None:\n            version = meta.get_latest_version()\n\n        # Verify that the version exists\n        matching_version_books = meta.get_version(version)\n        if not matching_version_books:\n            raise UnknownVersion(name, version)\n\n        # Find edition\n        if edition is None:\n            edition = matching_version_books[-1].edition\n        if edition not in [b.edition for b in matching_version_books]:\n            raise UnknownEdition(name, version, edition)\n        return version, edition\n\n    def list_versions(self, name: str) -&gt; list[str]:\n        \"\"\"\n        Get a list of available versions for a given Book\n\n        Parameters\n        ----------\n        name: str\n            Name of book\n\n        Returns\n        -------\n        list of str\n            List of available versions\n        \"\"\"\n        try:\n            meta = fetch_volume_meta(name, self.remote_bookshelf, self.path)\n        except requests.exceptions.HTTPError as http_error:\n            raise UnknownBook(f\"No metadata for {name!r}\") from http_error\n\n        return [version.version for version in meta.versions if not version.private]\n\n    def list_books(self) -&gt; list[str]:\n        \"\"\"\n        Get a list of book names\n\n        Returns\n        -------\n        list of str\n            List of available books\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/bookshelf/shelf/#bookshelf.shelf.BookShelf.is_available","title":"<code>is_available(name, version=None, edition=None)</code>","text":"<p>Check if a Book is available from the remote bookshelf</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the volume to check</p> required <code>version</code> <code>str</code> <p>Version of the volume to check</p> <p>If no version is provided, then check if any Book's with a matching name have been uploaded.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if a Book with a matching name and version exists on the remote bookshelf</p> Source code in <code>packages/bookshelf/src/bookshelf/shelf.py</code> <pre><code>def is_available(\n    self,\n    name: str,\n    version: Version | None = None,\n    edition: Edition | None = None,\n) -&gt; bool:\n    \"\"\"\n    Check if a Book is available from the remote bookshelf\n\n    Parameters\n    ----------\n    name : str\n        Name of the volume to check\n    version : str\n        Version of the volume to check\n\n        If no version is provided, then check if any Book's with a matching name\n        have been uploaded.\n\n    Returns\n    -------\n    bool\n        True if a Book with a matching name and version exists on the remote bookshelf\n    \"\"\"\n    try:\n        self._resolve_version(name, version, edition)\n    except (UnknownBook, UnknownVersion, UnknownEdition):\n        return False\n    return True\n</code></pre>"},{"location":"api/bookshelf/shelf/#bookshelf.shelf.BookShelf.is_cached","title":"<code>is_cached(name, version, edition)</code>","text":"<p>Check if a book with a matching name/version is cached on the local bookshelf</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the volume to check</p> required <code>version</code> <code>str</code> <p>Version of the volume to check</p> required <code>edition</code> <code>int</code> <p>Edition of the volume to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if a matching book is cached locally</p> Source code in <code>packages/bookshelf/src/bookshelf/shelf.py</code> <pre><code>def is_cached(self, name: str, version: Version, edition: Edition) -&gt; bool:\n    \"\"\"\n    Check if a book with a matching name/version is cached on the local bookshelf\n\n    Parameters\n    ----------\n    name : str\n        Name of the volume to check\n    version : str\n        Version of the volume to check\n    edition : int\n        Edition of the volume to check\n\n    Returns\n    -------\n    bool\n        True if a matching book is cached locally\n    \"\"\"\n    try:\n        # Check if the metadata for the book can be successfully read\n        book = LocalBook(name, version, edition, local_bookshelf=self.path)\n        book.metadata()\n    except FileNotFoundError:\n        return False\n    return True\n</code></pre>"},{"location":"api/bookshelf/shelf/#bookshelf.shelf.BookShelf.list_books","title":"<code>list_books()</code>","text":"<p>Get a list of book names</p> <p>Returns:</p> Type Description <code>list of str</code> <p>List of available books</p> Source code in <code>packages/bookshelf/src/bookshelf/shelf.py</code> <pre><code>def list_books(self) -&gt; list[str]:\n    \"\"\"\n    Get a list of book names\n\n    Returns\n    -------\n    list of str\n        List of available books\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/bookshelf/shelf/#bookshelf.shelf.BookShelf.list_versions","title":"<code>list_versions(name)</code>","text":"<p>Get a list of available versions for a given Book</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of book</p> required <p>Returns:</p> Type Description <code>list of str</code> <p>List of available versions</p> Source code in <code>packages/bookshelf/src/bookshelf/shelf.py</code> <pre><code>def list_versions(self, name: str) -&gt; list[str]:\n    \"\"\"\n    Get a list of available versions for a given Book\n\n    Parameters\n    ----------\n    name: str\n        Name of book\n\n    Returns\n    -------\n    list of str\n        List of available versions\n    \"\"\"\n    try:\n        meta = fetch_volume_meta(name, self.remote_bookshelf, self.path)\n    except requests.exceptions.HTTPError as http_error:\n        raise UnknownBook(f\"No metadata for {name!r}\") from http_error\n\n    return [version.version for version in meta.versions if not version.private]\n</code></pre>"},{"location":"api/bookshelf/shelf/#bookshelf.shelf.BookShelf.load","title":"<code>load(name, version=None, edition=None, force=False)</code>","text":"<p>Load a book</p> <p>If the book's metadata does not exist locally or an unknown version is requested the remote bookshelf is queried, otherwise the local metadata is used.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the volume to load</p> required <code>version</code> <code>Version | None</code> <p>Version to load</p> <p>If no version is provided, the latest version is returned</p> <code>None</code> <code>edition</code> <code>Edition | None</code> <p>Edition of book to load</p> <p>If no edition is provided, the latest edition of the selected version is returned</p> <code>None</code> <code>force</code> <code>bool</code> <p>If True, redownload the book metadata</p> <code>False</code> <p>Raises:</p> Type Description <code>UnknownVersion</code> <p>The requested version is not available for the selected volume</p> <code>UnknownBook</code> <p>An invalid volume is requested</p> <p>Returns:</p> Type Description <code>class:`LocalBook`</code> <p>A book from which the resources can be accessed</p> Source code in <code>packages/bookshelf/src/bookshelf/shelf.py</code> <pre><code>def load(\n    self,\n    name: str,\n    version: Version | None = None,\n    edition: Edition | None = None,\n    force: bool = False,\n) -&gt; LocalBook:\n    \"\"\"\n    Load a book\n\n    If the book's metadata does not exist locally or an unknown version is requested\n    the remote bookshelf is queried, otherwise the local metadata is used.\n\n    Parameters\n    ----------\n    name: str\n        Name of the volume to load\n    version: str\n        Version to load\n\n        If no version is provided, the latest version is returned\n    edition: int\n        Edition of book to load\n\n        If no edition is provided, the latest edition of the selected version is returned\n    force: bool\n        If True, redownload the book metadata\n\n    Raises\n    ------\n    UnknownVersion\n        The requested version is not available for the selected volume\n    UnknownBook\n        An invalid volume is requested\n\n    Returns\n    -------\n    :class:`LocalBook`\n        A book from which the resources can be accessed\n    \"\"\"\n    if version is None or edition is None or force:\n        version, edition = self._resolve_version(name, version, edition)\n    metadata_fragment = LocalBook.relative_path(name, version, edition, \"datapackage.json\")\n    metadata_fname = self.path / metadata_fragment\n    if not metadata_fname.exists():\n        try:\n            url = build_url(\n                self.remote_bookshelf,\n                *LocalBook.path_parts(name, version, edition, \"datapackage.json\"),\n            )\n            fetch_file(\n                url,\n                local_fname=metadata_fname,\n                known_hash=None,\n                force=force,\n            )\n        except requests.exceptions.HTTPError as http_error:\n            raise UnknownVersion(name, version) from http_error\n\n    if not metadata_fname.exists():\n        raise AssertionError()\n    return LocalBook(name, version, edition, local_bookshelf=self.path)\n</code></pre>"},{"location":"api/bookshelf/shelf/#bookshelf.shelf.fetch_volume_meta","title":"<code>fetch_volume_meta(name, remote_bookshelf, local_bookshelf, force=True)</code>","text":"<p>Fetch information about the books available for a given volume</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the volume to fetch</p> required <code>remote_bookshelf</code> <code>str</code> <p>URL for the remote bookshelf</p> required <code>local_bookshelf</code> <code>Path</code> <p>Local path where downloaded books will be stored.</p> <p>Must be a writable directory</p> required <code>force</code> <code>bool</code> <p>If True metadata is always fetched from the remote bookshelf</p> <code>True</code> <p>Returns:</p> Type Description <code>VolumeMeta</code> Source code in <code>packages/bookshelf/src/bookshelf/shelf.py</code> <pre><code>def fetch_volume_meta(\n    name: str,\n    remote_bookshelf: str,\n    local_bookshelf: pathlib.Path,\n    force: bool = True,\n) -&gt; VolumeMeta:\n    \"\"\"\n    Fetch information about the books available for a given volume\n\n    Parameters\n    ----------\n    name : str\n        Name of the volume to fetch\n    remote_bookshelf : str\n        URL for the remote bookshelf\n    local_bookshelf : pathlib.Path\n        Local path where downloaded books will be stored.\n\n        Must be a writable directory\n    force: bool\n        If True metadata is always fetched from the remote bookshelf\n\n    Returns\n    -------\n    VolumeMeta\n    \"\"\"\n    fname = \"volume.json\"\n\n    local_fname = local_bookshelf / name / fname\n    url = build_url(remote_bookshelf, name, fname)\n\n    fetch_file(url, local_fname, force=force)\n\n    with open(str(local_fname)) as file_handle:\n        data = json.load(file_handle)\n\n    return VolumeMeta(**data)\n</code></pre>"},{"location":"api/bookshelf/utils/","title":"bookshelf.utils","text":""},{"location":"api/bookshelf/utils/#bookshelf.utils","title":"<code>bookshelf.utils</code>","text":"<p>Bookshelf utilities</p>"},{"location":"api/bookshelf/utils/#bookshelf.utils.build_url","title":"<code>build_url(bookshelf, *paths)</code>","text":"<p>Build a URL</p> <p>Parameters:</p> Name Type Description Default <code>bookshelf</code> <code>str</code> <p>The remote bookshelf</p> required <code>paths</code> <code>list of str</code> <p>A collection of paths that form the path of the URL</p> <code>()</code> <p>Returns:</p> Type Description <code>str</code> <p>The merged URL</p> Source code in <code>packages/bookshelf/src/bookshelf/utils.py</code> <pre><code>def build_url(bookshelf: str, *paths: str) -&gt; str:\n    \"\"\"\n    Build a URL\n\n    Parameters\n    ----------\n    bookshelf: str\n        The remote bookshelf\n    paths : list of str\n        A collection of paths that form the path of the URL\n\n    Returns\n    -------\n    str\n        The merged URL\n\n    \"\"\"\n    return \"/\".join([bookshelf, *paths])\n</code></pre>"},{"location":"api/bookshelf/utils/#bookshelf.utils.create_local_cache","title":"<code>create_local_cache(path=None)</code>","text":"<p>Prepare a cache directory</p> <p>Creates a writeable directory in a platform-specific way (see pooch.os_cache)</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path | None</code> <p>If provided, override the default cache location</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <code>Location of a writable local cache</code> Source code in <code>packages/bookshelf/src/bookshelf/utils.py</code> <pre><code>def create_local_cache(path: str | pathlib.Path | None = None) -&gt; pathlib.Path:\n    \"\"\"\n    Prepare a cache directory\n\n    Creates a writeable directory in a platform-specific way (see [pooch.os_cache][pooch.os_cache])\n\n    Parameters\n    ----------\n    path: str or path\n        If provided, override the default cache location\n\n    Returns\n    -------\n    pathlib.Path\n\n    Location of a writable local cache\n    \"\"\"\n    if path is None:\n        path = default_cache_location()\n\n    path = pooch.utils.cache_location(pathlib.Path(path) / DATA_FORMAT_VERSION)\n\n    pooch.utils.make_local_storage(path)\n\n    return pathlib.Path(path)  # type: ignore\n</code></pre>"},{"location":"api/bookshelf/utils/#bookshelf.utils.default_cache_location","title":"<code>default_cache_location()</code>","text":"<p>Determine the default cache location</p> <p>By default, local Books are stored in the default cache location unless overridden for a given <code>BookShelf</code>. The default cache location is determined using the BOOKSHELF_CACHE_LOCATION environment variable  or if that is not present, it falls back to an operating specific location.  This location is determined using platformdirs   and may look like the following:</p> <ul> <li>Mac: <code>~/Library/Caches/bookshelf</code></li> <li>Unix: <code>~/.cache/bookshelf</code> or the value of the <code>XDG_CACHE_HOME</code>   environment variable, if defined.</li> <li>Windows: <code>C:\\Users\\&lt;user&gt;\\AppData\\Local\\bookshelf\\Cache</code></li> </ul> <p>Returns:</p> Type Description <code>str</code> <p>The default cache location</p> Source code in <code>packages/bookshelf/src/bookshelf/utils.py</code> <pre><code>def default_cache_location() -&gt; str:\n    r\"\"\"\n    Determine the default cache location\n\n    By default,\n    local Books are stored in the default cache location unless overridden for a given\n    [`BookShelf`][bookshelf.BookShelf].\n    The default cache location is determined using the\n    [BOOKSHELF_CACHE_LOCATION](/configuration/#bookshelf_cache_location) environment variable\n     or if that is not present, it falls back to an operating specific location.\n     This location is determined using [platformdirs](https://platformdirs.readthedocs.io/en/latest/)\n      and may look like the following:\n\n    * Mac: `~/Library/Caches/bookshelf`\n    * Unix: `~/.cache/bookshelf` or the value of the `XDG_CACHE_HOME`\n      environment variable, if defined.\n    * Windows: `C:\\Users\\&lt;user&gt;\\AppData\\Local\\bookshelf\\Cache`\n\n    Returns\n    -------\n    str\n        The default cache location\n    \"\"\"\n    return os.environ.get(\n        \"BOOKSHELF_CACHE_LOCATION\",\n        platformdirs.user_cache_dir(\"bookshelf\", appauthor=False),\n    )\n</code></pre>"},{"location":"api/bookshelf/utils/#bookshelf.utils.download","title":"<code>download(url, local_fname, known_hash=None, progressbar=False, retry_count=0)</code>","text":"<p>Download a remote file using pooch's downloader</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL to download</p> required <code>local_fname</code> <code>Path</code> <p>Path where the result will be stored</p> required <code>known_hash</code> <code>str | None</code> <code>None</code> <code>progressbar</code> <code>bool</code> <p>If true, show a progress bar showing the download process</p> <code>False</code> <code>retry_count</code> <code>int</code> <p>The number of retries to attempt</p> <code>0</code> Source code in <code>packages/bookshelf/src/bookshelf/utils.py</code> <pre><code>def download(\n    url: str,\n    local_fname: pathlib.Path,\n    known_hash: str | None = None,\n    progressbar: bool = False,\n    retry_count: int = 0,\n) -&gt; None:\n    \"\"\"\n    Download a remote file using pooch's downloader\n\n    Parameters\n    ----------\n    url: str\n        URL to download\n    local_fname\n        Path where the result will be stored\n    known_hash\n    progressbar: bool\n        If true, show a progress bar showing the download process\n    retry_count: int\n        The number of retries to attempt\n    \"\"\"\n    downloader = pooch.core.choose_downloader(url, progressbar=progressbar)\n    pooch.core.stream_download(\n        url,\n        fname=local_fname,\n        known_hash=known_hash,\n        downloader=downloader,\n        retry_if_failed=retry_count,\n    )\n</code></pre>"},{"location":"api/bookshelf/utils/#bookshelf.utils.fetch_file","title":"<code>fetch_file(url, local_fname, known_hash=None, force=False)</code>","text":"<p>Fetch a remote file and store it locally</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL of data file</p> required <code>local_fname</code> <code>Path</code> <p>The location of where to store the downloaded file</p> required <code>known_hash</code> <code>str</code> <p>Expected sha256 has of the output file.</p> <p>If the hash of the downloaded file doesn't match the expected hash a ValueError is raised.</p> <p>If no hash is provided, no checks are performed</p> <code>None</code> <code>force</code> <code>bool</code> <p>If True, always download the file</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Failing hash check for the output file</p> <code>FileNotFoundError</code> <p>Downloaded file was not in the expected location</p> Source code in <code>packages/bookshelf/src/bookshelf/utils.py</code> <pre><code>def fetch_file(\n    url: str,\n    local_fname: pathlib.Path,\n    known_hash: str | None = None,\n    force: bool | None = False,\n) -&gt; None:\n    \"\"\"\n    Fetch a remote file and store it locally\n\n    Parameters\n    ----------\n    url : str\n        URL of data file\n    local_fname : pathlib.Path\n        The location of where to store the downloaded file\n    known_hash : str\n        Expected sha256 has of the output file.\n\n        If the hash of the downloaded file doesn't match the expected hash a ValueError\n        is raised.\n\n        If no hash is provided, no checks are performed\n    force : bool\n        If True, always download the file\n\n    Raises\n    ------\n    ValueError\n        Failing hash check for the output file\n    FileNotFoundError\n        Downloaded file was not in the expected location\n\n    \"\"\"\n    if not force and local_fname.exists():\n        if pooch.hashes.hash_matches(local_fname, known_hash):\n            return\n        raise ValueError(\n            f\"Hash for existing file {local_fname} does not match the expected \" f\"value {known_hash}\"\n        )\n\n    if force or not local_fname.exists():\n        download(url, local_fname=local_fname, known_hash=known_hash)\n        logger.info(f\"{local_fname} downloaded from {url}\")\n\n    if not local_fname.exists():\n        raise FileNotFoundError(f\"Could not find file {local_fname}\")  # pragma: no cover\n</code></pre>"},{"location":"api/bookshelf/utils/#bookshelf.utils.get_env_var","title":"<code>get_env_var(name, add_prefix=True, raise_on_missing=True, default=None)</code>","text":"<p>Get an environment variable value</p> <p>If the variable isn't set raise an exception if <code>raise_on_missing</code> is <code>True</code></p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Environment variable name to check</p> required <code>add_prefix</code> <code>bool</code> <p>If <code>True</code>, prefix the environment variable name with ENV_PREFIX</p> <code>True</code> <code>raise_on_missing</code> <code>bool</code> <p>If <code>True</code>, a ValueError is raised</p> <code>True</code> <code>default</code> <code>Any</code> <p>Value to return if the environment variable is missing and <code>raise_on_missing</code> is <code>True</code></p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Value of environment variable</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Environment variable not set</p> Source code in <code>packages/bookshelf/src/bookshelf/utils.py</code> <pre><code>def get_env_var(\n    name: str,\n    add_prefix: bool = True,\n    raise_on_missing: bool = True,\n    default: Any = None,\n) -&gt; str:\n    \"\"\"\n    Get an environment variable value\n\n    If the variable isn't set raise an exception if `raise_on_missing` is `True`\n\n    Parameters\n    ----------\n    name : str\n        Environment variable name to check\n    add_prefix : bool\n        If `True`, prefix the environment variable name with\n        [ENV_PREFIX][bookshelf.constants.ENV_PREFIX]\n    raise_on_missing : bool\n        If `True`, a ValueError is raised\n    default : Any\n        Value to return if the environment variable is missing and `raise_on_missing` is `True`\n\n    Returns\n    -------\n    str\n        Value of environment variable\n\n    Raises\n    ------\n    ValueError\n        Environment variable not set\n    \"\"\"\n    if add_prefix:\n        name = ENV_PREFIX + name\n    name = name.upper()\n    if raise_on_missing and default is None and name not in os.environ:\n        raise ValueError(f\"Environment variable {name} not set. Check configuration\")\n    return os.environ.get(name, default)\n</code></pre>"},{"location":"api/bookshelf/utils/#bookshelf.utils.get_notebook_directory","title":"<code>get_notebook_directory(nb_dir=None)</code>","text":"<p>Get the root location of the notebooks used to generate books</p> <p>The order of lookup is (in increasing precedence):</p> <ul> <li>default (\"/notebooks\" in a locally checked out version of the repository)</li> <li>BOOKSHELF_NOTEBOOK_DIRECTORY environment variable</li> <li><code>nb_dir</code> parameter</li> </ul> <p>Parameters:</p> Name Type Description Default <code>nb_dir</code> <code>str</code> <p>If provided override the default value</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Location of notebooks</p> Source code in <code>packages/bookshelf/src/bookshelf/utils.py</code> <pre><code>def get_notebook_directory(nb_dir: str | None = None) -&gt; str:\n    \"\"\"\n    Get the root location of the notebooks used to generate books\n\n    The order of lookup is (in increasing precedence):\n\n    * default (\"/notebooks\" in a locally checked out version of the repository)\n    * [BOOKSHELF_NOTEBOOK_DIRECTORY](/configuration/#bookshelf_notebook_directory) environment variable\n    * `nb_dir` parameter\n\n    Parameters\n    ----------\n    nb_dir : str\n        If provided override the default value\n\n    Returns\n    -------\n    str\n        Location of notebooks\n\n    \"\"\"\n    if nb_dir:\n        return nb_dir\n\n    try:\n        nb_directory = get_env_var(\"NOTEBOOK_DIRECTORY\")\n    except ValueError:\n        nb_directory = os.path.join(ROOT_DIR, \"notebooks\")\n\n    return nb_directory\n</code></pre>"},{"location":"api/bookshelf/utils/#bookshelf.utils.get_remote_bookshelf","title":"<code>get_remote_bookshelf(bookshelf)</code>","text":"<p>Get the remote bookshelf URL</p> <p>If no bookshelf is provided, use the BOOKSHELF_REMOTE environment variable, or, the DEFAULT_BOOKSHELF parameter if the environment variable is not present.</p> <p>Parameters:</p> Name Type Description Default <code>bookshelf</code> <code>str</code> <p>URL for the bookshelf</p> <p>If not provided the URL is determined as above</p> required <p>Returns:</p> Type Description <code>str</code> <p>URL for the remote bookshelf</p> Source code in <code>packages/bookshelf/src/bookshelf/utils.py</code> <pre><code>def get_remote_bookshelf(bookshelf: str | None) -&gt; str:\n    \"\"\"\n    Get the remote bookshelf URL\n\n    If no bookshelf is provided,\n    use the [BOOKSHELF_REMOTE](/configuration/#bookshelf_remote) environment variable, or,\n    the [DEFAULT_BOOKSHELF][bookshelf.constants.DEFAULT_BOOKSHELF] parameter\n    if the environment variable is not present.\n\n    Parameters\n    ----------\n    bookshelf : str\n        URL for the bookshelf\n\n        If not provided the URL is determined as above\n\n    Returns\n    -------\n    str\n        URL for the remote bookshelf\n    \"\"\"\n    if bookshelf is None:\n        return os.environ.get(ENV_PREFIX + \"REMOTE\", DEFAULT_BOOKSHELF)\n    return bookshelf\n</code></pre>"},{"location":"api/bookshelf_producer/","title":"bookshelf_producer","text":"Sub-package Description actions Actions that can be performed on the bookshelf cli Bookshelf CLI commands CLI commands constants Producer constants notebook Functions to run/manage notebooks"},{"location":"api/bookshelf_producer/#bookshelf_producer","title":"<code>bookshelf_producer</code>","text":"<p>Package for producing new bookshelf data</p>"},{"location":"api/bookshelf_producer/actions/","title":"bookshelf_producer.actions","text":""},{"location":"api/bookshelf_producer/actions/#bookshelf_producer.actions","title":"<code>bookshelf_producer.actions</code>","text":"<p>Actions that can be performed on the bookshelf</p>"},{"location":"api/bookshelf_producer/actions/#bookshelf_producer.actions.publish","title":"<code>publish(shelf, book, force=False)</code>","text":"<p>Publish a book to the remote bookshelf</p> <p>Parameters:</p> Name Type Description Default <code>book</code> <code>LocalBook</code> <p>Book to upload</p> required <code>force</code> <code>bool</code> <p>If True, overwrite any existing data</p> <code>False</code> <p>Raises:</p> Type Description <code>UploadError</code> <p>Unable to upload the book to the remote bookshelf. See error message for more information about how to resolve this issue.</p> Source code in <code>packages/bookshelf-producer/src/bookshelf_producer/actions.py</code> <pre><code>def publish(shelf: BookShelf, book: LocalBook, force: bool = False) -&gt; None:\n    \"\"\"\n    Publish a book to the remote bookshelf\n\n    Parameters\n    ----------\n    book : LocalBook\n        Book to upload\n    force : bool\n        If True, overwrite any existing data\n\n    Raises\n    ------\n    UploadError\n        Unable to upload the book to the remote bookshelf. See error message for\n        more information about how to resolve this issue.\n    \"\"\"\n    if shelf.is_available(book.name, book.version):\n        remote_book = shelf.load(book.name, book.version)\n\n        if remote_book.edition &gt;= book.edition:\n            msg = (\n                \"Edition value has not been increased (remote:\"\n                f\" {remote_book.long_version()}, local: {book.long_version()})\"\n            )\n            if not force:\n                raise UploadError(msg)\n            logger.error(msg)\n        logger.warning(\"Uploading a new edition of an existing book\")\n    files = book.files()\n\n    # Check if additional files are going to be uploaded\n    resources = book.as_datapackage().resources\n    resource_fnames = [\n        resource.descriptor[\"filename\"] for resource in cast(Iterable[datapackage.Resource], resources)\n    ]\n    for resource_file in files:\n        fname = os.path.basename(resource_file)\n        if fname == \"datapackage.json\":\n            continue\n        if fname not in resource_fnames:\n            raise UploadError(f\"Non-resource file {fname} found in book\")\n\n    # Upload using boto3 by default for testing\n    # Maybe support other upload methods in future\n\n    s3 = boto3.client(\"s3\")\n    bucket = get_env_var(\"BUCKET\", add_prefix=True, default=DEFAULT_S3_BUCKET)\n    prefix = get_env_var(\"BUCKET_PREFIX\", add_prefix=True, default=DATA_FORMAT_VERSION)\n\n    logger.info(f\"Beginning to upload {book.name}@{book.version}\")\n    for resource_file in files:\n        key = \"/\".join(\n            (\n                prefix,\n                book.name,\n                book.long_version(),\n                os.path.basename(resource_file),\n            )\n        )\n        _upload_file(s3, bucket, key, resource_file)\n\n    # Update the metadata with the latest version information\n    # Note that this doesn't have any guardrails and is susceptible to race conditions\n    # Shouldn't be a problem for testing, but shouldn't be used in production\n    meta_fname = _update_volume_meta(book, shelf.remote_bookshelf)\n    key = \"/\".join((prefix, book.name, os.path.basename(meta_fname)))\n    _upload_file(s3, bucket, key, meta_fname)\n\n    logger.info(f\"Book {book.name}@{book.version} ed.{book.edition} uploaded successfully\")\n</code></pre>"},{"location":"api/bookshelf_producer/cli/","title":"bookshelf_producer.cli","text":""},{"location":"api/bookshelf_producer/cli/#bookshelf_producer.cli","title":"<code>bookshelf_producer.cli</code>","text":"<p>Bookshelf CLI</p>"},{"location":"api/bookshelf_producer/cli/#bookshelf_producer.cli.main","title":"<code>main(ctx, quiet)</code>","text":"<p>Bookshelf for managing reusable datasets</p> Source code in <code>packages/bookshelf-producer/src/bookshelf_producer/cli.py</code> <pre><code>@click.command(cls=_CLICommands, name=\"bookshelf\")\n@click.option(\"-q\", \"--quiet\", is_flag=True)\n@click_log.simple_verbosity_option(logger)  # type: ignore\n@click.pass_context\ndef main(ctx, quiet) -&gt; None:\n    \"\"\"\n    Bookshelf for managing reusable datasets\n    \"\"\"\n    ctx.ensure_object(dict)\n\n    if not logger.hasHandlers():\n        click_log.basic_config(logger)  # pragma: no cover\n\n    logger.setLevel(logging.INFO)\n    if quiet:\n        logger.setLevel(logging.ERROR)\n</code></pre>"},{"location":"api/bookshelf_producer/commands/","title":"bookshelf_producer.commands","text":"Sub-package Description cmd_publish publish CLI command cmd_run run CLI command"},{"location":"api/bookshelf_producer/commands/#bookshelf_producer.commands","title":"<code>bookshelf_producer.commands</code>","text":"<p>CLI commands</p>"},{"location":"api/bookshelf_producer/commands/cmd_publish/","title":"bookshelf_producer.commands.cmd_publish","text":""},{"location":"api/bookshelf_producer/commands/cmd_publish/#bookshelf_producer.commands.cmd_publish","title":"<code>bookshelf_producer.commands.cmd_publish</code>","text":"<p>publish CLI command</p>"},{"location":"api/bookshelf_producer/commands/cmd_publish/#bookshelf_producer.commands.cmd_publish.cli","title":"<code>cli(name, version, include_private, force)</code>","text":"<p>Build and upload a Book to the Bookshelf</p> <p>Uploading a Book requires the correct AWS credentials (until an authentication scheme is introduced). At Climate Resource we use aws-vault for managing multiple sets of AWS credentials. Documentation about the different sources of authentication can be found here: https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html</p> <p>To ensure a reproducible build, the Book is built from a notebook in an isolated output directory. There currently isn't any functionality to upload a pre-built Book.</p> Source code in <code>packages/bookshelf-producer/src/bookshelf_producer/commands/cmd_publish.py</code> <pre><code>@click.command(\"publish\", short_help=\"Upload a book to the bookshelf\")\n@click.argument(\"name\", required=True)\n@click.option(\n    \"--version\",\n    multiple=True,\n    help=\"List of versions to run\",\n    required=False,\n)\n@click.option(\n    \"--include-private/--no-include-private\",\n    help=\"Run private versions. These will likely fail if the data is not available locally\",\n    default=False,\n)\n@click.option(\n    \"--force\",\n    is_flag=True,\n    help=\"Override the existing published data\",\n    default=False,\n)\ndef cli(name: str, version: tuple[str, ...], include_private: bool, force: bool) -&gt; None:\n    \"\"\"\n    Build and upload a Book to the Bookshelf\n\n    Uploading a Book requires the correct AWS credentials (until an authentication\n    scheme is introduced). At Climate Resource we use aws-vault for managing multiple\n    sets of AWS credentials. Documentation about the different sources of authentication\n    can be found here: [https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html]()\n\n    To ensure a reproducible build, the Book is built from a notebook in an isolated\n    output directory. There currently isn't any functionality to upload a pre-built Book.\n    \"\"\"\n    if not version:\n        all_versions = get_available_versions(name, include_private=include_private)\n    else:\n        all_versions = version\n\n    for dataset_version in all_versions:\n        with tempfile.TemporaryDirectory() as temp_dir:\n            logger.info(f\"Building Book in isolated environment: {temp_dir}\")\n            try:\n                book = run_notebook(\n                    name,\n                    output_directory=temp_dir,\n                    force=False,\n                    version=dataset_version,\n                )\n                logger.info(f\"Finish building {name}@{book.long_version()}\")\n\n                shelf = BookShelf()\n                publish(shelf, book, force=force)\n            except Exception as exc:\n                logger.exception(f\"Unable to process {name}@{dataset_version}\")\n                raise click.Abort() from exc\n</code></pre>"},{"location":"api/bookshelf_producer/commands/cmd_run/","title":"bookshelf_producer.commands.cmd_run","text":""},{"location":"api/bookshelf_producer/commands/cmd_run/#bookshelf_producer.commands.cmd_run","title":"<code>bookshelf_producer.commands.cmd_run</code>","text":"<p>run CLI command</p>"},{"location":"api/bookshelf_producer/commands/cmd_run/#bookshelf_producer.commands.cmd_run.cli","title":"<code>cli(name, output, force, version, include_private)</code>","text":"<p>Run a notebook</p> <p>This runs one of the notebooks used to generate a Book</p> Source code in <code>packages/bookshelf-producer/src/bookshelf_producer/commands/cmd_run.py</code> <pre><code>@click.command(\"run\", short_help=\"Run a notebook\")\n@click.argument(\"name\", required=True)\n@click.option(\n    \"-o\",\n    \"--output\",\n    help=\"Directory to store the artifacts from running the notebook\",\n    required=False,\n)\n@click.option(\n    \"--version\",\n    multiple=True,\n    help=\"List of versions to run\",\n    required=False,\n)\n@click.option(\n    \"--include-private/--no-include-private\",\n    help=\"Run private versions. These will likely fail if the data is not available locally\",\n    default=False,\n)\n@click.option(\n    \"-f\",\n    \"--force\",\n    help=\"Override the existing output if the output directory isn't empty\",\n    is_flag=True,\n)\ndef cli(name: str, output: str, force: bool, version: tuple[str], include_private: bool) -&gt; None:\n    \"\"\"\n    Run a notebook\n\n    This runs one of the notebooks used to generate a Book\n    \"\"\"\n    if not len(version):\n        all_versions = get_available_versions(name, include_private=include_private)\n    else:\n        all_versions = version\n\n    for dataset_version in all_versions:\n        try:\n            run_notebook(name, output_directory=output, force=force, version=dataset_version)\n        except Exception as exc:\n            logger.error(f\"Failed to run {name}: {exc}\")\n            raise click.Abort() from exc\n</code></pre>"},{"location":"api/bookshelf_producer/constants/","title":"bookshelf_producer.constants","text":""},{"location":"api/bookshelf_producer/constants/#bookshelf_producer.constants","title":"<code>bookshelf_producer.constants</code>","text":"<p>Producer constants</p>"},{"location":"api/bookshelf_producer/constants/#bookshelf_producer.constants.DEFAULT_S3_BUCKET","title":"<code>DEFAULT_S3_BUCKET = 'cr-prod-datasets-bookshelf'</code>  <code>module-attribute</code>","text":"<p>The default bucket used to store the raw data files</p>"},{"location":"api/bookshelf_producer/notebook/","title":"bookshelf_producer.notebook","text":""},{"location":"api/bookshelf_producer/notebook/#bookshelf_producer.notebook","title":"<code>bookshelf_producer.notebook</code>","text":"<p>Functions to run/manage notebooks</p>"},{"location":"api/bookshelf_producer/notebook/#bookshelf_producer.notebook.get_available_versions","title":"<code>get_available_versions(name, include_private=False)</code>","text":"<p>Get a list of available versions of a book</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Package name</p> required <code>include_private</code> <code>bool</code> <p>If True, also include private versions</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[str, ...]</code> <p>List of versions</p> Source code in <code>packages/bookshelf-producer/src/bookshelf_producer/notebook.py</code> <pre><code>def get_available_versions(name: str, include_private: bool = False) -&gt; tuple[str, ...]:\n    \"\"\"\n    Get a list of available versions of a book\n\n    Parameters\n    ----------\n    name\n        Package name\n    include_private\n        If True, also include private versions\n\n    Returns\n    -------\n    :\n        List of versions\n    \"\"\"\n    config, _ = _load_nb_config(name)\n\n    versions = config.versions\n    if not include_private:\n        versions = [v for v in versions if not v.private]\n\n    return tuple(v.version for v in versions)\n</code></pre>"},{"location":"api/bookshelf_producer/notebook/#bookshelf_producer.notebook.load_nb_metadata","title":"<code>load_nb_metadata(name, version=None, nb_directory=None)</code>","text":"<p>Load notebook metadata</p> <p>Attempts to search <code>nb_directory</code> for a metadata YAML file. This YAML file contains information about the dataset that is being processed. See NotebookMetadata for a description of the available options.</p> <p>The assumed filename format for versioned data is <code>{name}_{version}.yaml</code> where name matches the notebook name and the name as specified in the NotebookMetadata</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Filename to load. Should match the notebook name (not checked)</p> required <code>version</code> <code>str</code> <p>Version of the metadata to load. If none is provided, the last version will be used</p> <code>None</code> <code>nb_directory</code> <code>str | None</code> <p>If a non-absolute path is provided, it is assumed to be relative to nb_directory</p> <code>None</code> <p>Raises:</p> Type Description <code>UnknownVersion</code> <p>A matching version is not in the configuration</p> <p>Returns:</p> Type Description <code>NotebookMetadata</code> <p>Metadata about the notebook including the target package and version</p> Source code in <code>packages/bookshelf-producer/src/bookshelf_producer/notebook.py</code> <pre><code>def load_nb_metadata(\n    name: str,\n    version: Version | None = None,\n    nb_directory: str | None = None,\n) -&gt; NotebookMetadata:\n    \"\"\"\n    Load notebook metadata\n\n    Attempts to search `nb_directory` for a metadata YAML file. This YAML file\n    contains information about the dataset that is being processed. See NotebookMetadata\n    for a description of the available options.\n\n    The assumed filename format for versioned data is `{name}_{version}.yaml` where\n    name matches the notebook name and the name as specified in the NotebookMetadata\n\n    Parameters\n    ----------\n    name : str\n        Filename to load. Should match the notebook name (not checked)\n    version : str\n        Version of the metadata to load. If none is provided, the last version will be used\n    nb_directory: str\n        If a non-absolute path is provided, it is assumed to be relative to nb_directory\n\n    Raises\n    ------\n    UnknownVersion\n        A matching version is not in the configuration\n\n    Returns\n    -------\n    NotebookMetadata\n        Metadata about the notebook including the target package and version\n    \"\"\"\n    config, raw_data = _load_nb_config(name, nb_directory)\n\n    if \"version\" in raw_data and version != raw_data[\"version\"]:\n        # Check if a version has already been selected\n        raise ValueError(\"Requested version does not match the metadata\")\n\n    if version:\n        selected_version = None\n        for v in config.versions:\n            if v.version == version:\n                selected_version = v\n    else:\n        selected_version = config.versions[-1]\n    if selected_version is None:\n        raise UnknownVersion(config.name, version)\n\n    return NotebookMetadata(**raw_data, **selected_version.dict())\n</code></pre>"},{"location":"api/bookshelf_producer/notebook/#bookshelf_producer.notebook.run_notebook","title":"<code>run_notebook(name, nb_directory=None, output_directory=None, force=False, version=None)</code>","text":"<p>Run a notebook to generate a new Book</p> <p>The jupytext <code>.py</code> version of the notebook is used.</p> <p>The template file and configuration is copied to the output directory. The template <code>.py</code> file is then used to create a notebook which is run using <code>papermill</code>. The <code>local_bookshelf</code> parameter is also set to the output directory.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the notebook</p> required <code>nb_directory</code> <code>str</code> <p>Directory containing the notebooks.</p> <p>This defaults to the <code>notebooks/</code> directory in this project</p> <code>None</code> <code>output_directory</code> <code>str</code> <p>Where the output directory will be created.</p> <p>This defaults to <code>data/processing/{name}/{version}</code></p> <code>None</code> <code>force</code> <code>bool</code> <p>If True, override the existing data in the output directory</p> <code>False</code> <code>version</code> <code>str</code> <p>Version to extract</p> <code>None</code> <p>Returns:</p> Type Description <code>LocalBook</code> <p>The generated book</p> Source code in <code>packages/bookshelf-producer/src/bookshelf_producer/notebook.py</code> <pre><code>def run_notebook(\n    name: str,\n    nb_directory: str | None = None,\n    output_directory: str | None = None,\n    force: bool = False,\n    version: Version | None = None,\n) -&gt; LocalBook:\n    \"\"\"\n    Run a notebook to generate a new Book\n\n    The jupytext `.py` version of the notebook is used.\n\n    The template file and configuration is copied to the output directory. The\n    template `.py` file is then used to create a notebook which is run using\n    `papermill`. The `local_bookshelf` parameter is also set to the output\n    directory.\n\n    Parameters\n    ----------\n    name : str\n        Name of the notebook\n    nb_directory : str\n        Directory containing the notebooks.\n\n        This defaults to the `notebooks/` directory in this project\n    output_directory : str\n        Where the output directory will be created.\n\n        This defaults to `data/processing/{name}/{version}`\n    force : bool\n        If True, override the existing data in the output directory\n    version : str\n        Version to extract\n\n    Returns\n    -------\n    :\n        The generated book\n    \"\"\"\n    if not has_papermill:\n        raise ImportError(\"papermill is not installed. Run 'pip install bookshelf[notebooks]'\")\n    if not has_jupytext:\n        raise ImportError(\"jupytext is not installed. Run 'pip install bookshelf[notebooks]'\")\n\n    short_name = name.split(\"/\")[-1]\n\n    # Verify metadata\n    metadata = load_nb_metadata(name, version=version, nb_directory=nb_directory)\n    nb_fname = metadata.source_file.replace(\".yaml\", \".py\")\n\n    if not os.path.exists(nb_fname):\n        raise FileNotFoundError(f\"Could not find notebook: {nb_fname}\")\n\n    logger.info(f\"Loaded metadata from {metadata.source_file}\")\n    if metadata.name != short_name:  # pragma: no cover\n        raise ValueError(\n            \"name in metadata does not match the name of the notebook \" f\"({metadata.name} != {name}\"\n        )\n    logger.info(f\"Processing {metadata.long_name()}\")\n\n    if output_directory is None:\n        output_directory = os.path.join(PROCESSED_DATA_DIR, short_name)\n\n    output_directory = os.path.join(output_directory, metadata.version)\n    if os.path.exists(output_directory) and os.listdir(output_directory):\n        logger.warning(f\"{output_directory} is not empty\")\n        if not force:\n            raise ValueError(f\"{output_directory} is not empty\")\n    os.makedirs(output_directory, exist_ok=True)\n\n    # Copy required files\n    logger.info(f\"Copying {nb_fname} to {output_directory}\")\n    shutil.copyfile(nb_fname, os.path.join(output_directory, f\"{short_name}.py\"))\n    logger.info(f\"Copying metadata to {output_directory}\")\n    with open(os.path.join(output_directory, f\"{short_name}.yaml\"), \"w\") as fh:\n        yaml.safe_dump(metadata.dict(), fh)\n\n    # Template and run notebook\n    output_nb_fname = os.path.join(output_directory, f\"{short_name}.ipynb\")\n    logger.info(f\"Creating notebook {output_nb_fname} from {nb_fname}\")\n    notebook_jupytext = jupytext.read(nb_fname)\n    jupytext.write(\n        notebook_jupytext,\n        output_nb_fname,\n        fmt=\"ipynb\",\n    )\n    papermill.execute_notebook(\n        output_nb_fname,\n        output_nb_fname,\n        parameters={\"local_bookshelf\": output_directory, \"version\": version},\n    )\n    # Attempt to load the book from the output directory\n    shelf = BookShelf(path=output_directory)\n    book = shelf.load(short_name, metadata.version, edition=metadata.edition)\n    logger.info(f\"Notebook run successfully with hash: {book.hash()}\")\n    return book\n</code></pre>"},{"location":"how-to-guides/","title":"How-to guides","text":"<p>This part of the project documentation focuses on a problem-oriented approach. We'll go over how to solve common tasks.</p>"},{"location":"how-to-guides/#data-consumers","title":"Data consumers","text":"<ul> <li>\"Example analysis\"</li> <li>\"Find all available books\"</li> </ul>"},{"location":"how-to-guides/#data-producers","title":"Data producers","text":"<ul> <li>\"How do I create a new volume?\"\"</li> </ul>"},{"location":"how-to-guides/analysis_with_a_book/","title":"Analysis","text":"In\u00a0[1]: Copied! <pre>import warnings\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nfrom matplotlib.pyplot import figure\n\nfrom bookshelf import BookShelf\n\nshelf = BookShelf()\n\nvolume = \"rcmip-emissions\"\nversion = \"v5.1.0\"\nbook = shelf.load(volume, version)\n</pre> import warnings  warnings.filterwarnings(\"ignore\", category=FutureWarning) from matplotlib.pyplot import figure  from bookshelf import BookShelf  shelf = BookShelf()  volume = \"rcmip-emissions\" version = \"v5.1.0\" book = shelf.load(volume, version) <pre>/home/runner/work/bookshelf/bookshelf/.venv/lib/python3.10/site-packages/scmdata/database/_database.py:9: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  import tqdm.autonotebook as tqdman\n</pre> <p>Once the book is loaded, access specific timeseries data in a wide format by using the <code>timeseries</code> function and specifying the book name. This data will be returned as an <code>scmdata.ScmRun</code> object. Alternatively, use the <code>get_long_format_data</code> function to obtain timeseries data in a long format, which returns a <code>pd.DataFrame</code> object:</p> In\u00a0[2]: Copied! <pre>data_wide = book.timeseries(\"complete\")\n# data_long = book.get_long_format_data(\"complete\")\n</pre> data_wide = book.timeseries(\"complete\") # data_long = book.get_long_format_data(\"complete\") In\u00a0[3]: Copied! <pre>data_wide.filter(variable=\"Emissions|CO2|MAGICC AFOLU\")\n</pre> data_wide.filter(variable=\"Emissions|CO2|MAGICC AFOLU\") Out[3]: <pre>&lt;ScmRun (timeseries: 77, timepoints: 751)&gt;\nTime:\n\tStart: 1750-01-01T00:00:00\n\tEnd: 2500-01-01T00:00:00\nMeta:\n\t         activity_id mip_era          model         region          scenario  \\\n\t4             ZECMIP   CMIP6      idealised          World  esm-bell-1000PgC   \n\t44            ZECMIP   CMIP6      idealised          World  esm-bell-2000PgC   \n\t84            ZECMIP   CMIP6      idealised          World   esm-bell-750PgC   \n\t124   not_applicable   CMIP5            AIM          World             rcp60   \n\t164   not_applicable   CMIP5          IMAGE          World             rcp26   \n\t...              ...     ...            ...            ...               ...   \n\t8937  not_applicable   CMIP6  REMIND-MAGPIE  World|R5.2REF       ssp534-over   \n\t9061  not_applicable   CMIP6  REMIND-MAGPIE  World|R5.2REF            ssp585   \n\t9146  not_applicable   CMIP6      idealised          World   esm-pi-CO2pulse   \n\t9186  not_applicable   CMIP6      idealised          World  esm-pi-cdr-pulse   \n\t9226  not_applicable   CMIP6      idealised          World     esm-piControl   \n\t\n\t           unit                    variable  \n\t4     Mt CO2/yr  Emissions|CO2|MAGICC AFOLU  \n\t44    Mt CO2/yr  Emissions|CO2|MAGICC AFOLU  \n\t84    Mt CO2/yr  Emissions|CO2|MAGICC AFOLU  \n\t124   Mt CO2/yr  Emissions|CO2|MAGICC AFOLU  \n\t164   Mt CO2/yr  Emissions|CO2|MAGICC AFOLU  \n\t...         ...                         ...  \n\t8937  Mt CO2/yr  Emissions|CO2|MAGICC AFOLU  \n\t9061  Mt CO2/yr  Emissions|CO2|MAGICC AFOLU  \n\t9146  Mt CO2/yr  Emissions|CO2|MAGICC AFOLU  \n\t9186  Mt CO2/yr  Emissions|CO2|MAGICC AFOLU  \n\t9226  Mt CO2/yr  Emissions|CO2|MAGICC AFOLU  \n\t\n\t[77 rows x 7 columns]</pre> <p>For long format data, employ <code>pandas</code> functionality to apply necessary filters.</p> In\u00a0[4]: Copied! <pre>figure(figsize=(10, 6), dpi=160)\n\ndata_wide.filter(variable=\"Emissions|CO2|MAGICC AFOLU\").lineplot()\n</pre> figure(figsize=(10, 6), dpi=160)  data_wide.filter(variable=\"Emissions|CO2|MAGICC AFOLU\").lineplot() Out[4]: <pre>&lt;Axes: xlabel='time', ylabel='Mt CO2/yr'&gt;</pre> <p>This approach allows you to efficiently load, filter, and visualize datasets from your bookshelf, facilitating in-depth analysis and insights.</p>"},{"location":"how-to-guides/analysis_with_a_book/#book-data-analysis-example","title":"Book data analysis example\u00b6","text":""},{"location":"how-to-guides/analysis_with_a_book/#loading-a-dataset","title":"Loading a dataset\u00b6","text":"<p>Begin by initializing a <code>BookShelf</code> object. Specify the desired volume and version to retrieve the corresponding book:</p>"},{"location":"how-to-guides/analysis_with_a_book/#filtering-data","title":"Filtering data\u00b6","text":"<p>For data in wide format, use the <code>filter</code> method from ScmData to refine the dataset based on specific metadata criteria:</p>"},{"location":"how-to-guides/analysis_with_a_book/#plotting","title":"Plotting\u00b6","text":"<p>For wide format data, visualize your data using built-in plotting functions from ScmData. For instance, to generate a line plot based on filtered metadata:</p>"},{"location":"how-to-guides/create_a_new_volume/","title":"Create a new volume","text":"In\u00a0[1]: Copied! <pre>import logging\nimport tempfile\n\nfrom scmdata import testing\n\nfrom bookshelf import LocalBook\nfrom bookshelf_producer.notebook import load_nb_metadata\n\nlogging.basicConfig(level=logging.INFO)\n</pre> import logging import tempfile  from scmdata import testing  from bookshelf import LocalBook from bookshelf_producer.notebook import load_nb_metadata  logging.basicConfig(level=logging.INFO) <pre>/home/runner/work/bookshelf/bookshelf/.venv/lib/python3.10/site-packages/scmdata/database/_database.py:9: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  import tqdm.autonotebook as tqdman\n</pre> In\u00a0[2]: Copied! <pre>metadata = load_nb_metadata(\"example_volume/example_volume\")\nmetadata.dict()\n</pre> metadata = load_nb_metadata(\"example_volume/example_volume\") metadata.dict() Out[2]: <pre>{'name': 'example_volume',\n 'version': 'v0.1.0',\n 'edition': 1,\n 'description': None,\n 'license': 'MIT',\n 'source_file': '/home/runner/work/bookshelf/bookshelf/notebooks/example_volume/example_volume.yaml',\n 'private': False,\n 'metadata': {'author': 'Yini Lai',\n  'author_email': 'yini.lai@climate-resource.com'},\n 'dataset': {'url': None,\n  'doi': None,\n  'files': [{'url': 'https://rcmip-protocols-au.s3-ap-southeast-2.amazonaws.com/v5.1.0/rcmip-radiative-forcing-annual-means-v5-1-0.csv',\n    'hash': '15ef911f0ea9854847dcd819df300cedac5fd001c6e740f2c5fdb32761ddec8b'}],\n  'author': 'Zebedee Nicholls'},\n 'data_dictionary': [{'name': 'model',\n   'description': 'The IAM that was used to create the scenario',\n   'type': 'string',\n   'required_column': True,\n   'allowed_NA': True,\n   'controlled_vocabulary': None},\n  {'name': 'unit',\n   'description': 'Unit of the timeseries',\n   'type': 'string',\n   'required_column': True,\n   'allowed_NA': False,\n   'controlled_vocabulary': None},\n  {'name': 'scenario',\n   'description': 'scenario',\n   'type': 'string',\n   'required_column': True,\n   'allowed_NA': False,\n   'controlled_vocabulary': None},\n  {'name': 'region',\n   'description': 'Area that the results are valid for',\n   'type': 'string',\n   'required_column': True,\n   'allowed_NA': False,\n   'controlled_vocabulary': [{'value': 'World',\n     'description': 'Aggregate results for the world'}]},\n  {'name': 'variable',\n   'description': 'Variable name',\n   'type': 'string',\n   'required_column': True,\n   'allowed_NA': True,\n   'controlled_vocabulary': None}]}</pre> In\u00a0[3]: Copied! <pre>data = testing.get_single_ts()\ndata.timeseries()\n</pre> data = testing.get_single_ts() data.timeseries() Out[3]: time 0001-01-01 00:00:00 0002-01-01 00:00:00 0003-01-01 00:00:00 model region scenario unit variable mod World scen GtC / yr Emissions|CO2 1.0 2.0 3.0 In\u00a0[4]: Copied! <pre># create and return a unique temporary directory\nlocal_bookshelf = tempfile.mkdtemp()\nbook = LocalBook.create_from_metadata(metadata, local_bookshelf=local_bookshelf)\n</pre> # create and return a unique temporary directory local_bookshelf = tempfile.mkdtemp() book = LocalBook.create_from_metadata(metadata, local_bookshelf=local_bookshelf) In\u00a0[5]: Copied! <pre>book.add_timeseries(\"example_resource_name\", data)\n</pre> book.add_timeseries(\"example_resource_name\", data) <p>Display the <code>Book</code>'s metadata, which encompasses all metadata about the Book and its associated <code>Resources</code>:</p> In\u00a0[6]: Copied! <pre>book.metadata()\n</pre> book.metadata() Out[6]: <pre>{'name': 'example_volume',\n 'version': 'v0.1.0',\n 'private': False,\n 'edition': 1,\n 'resources': [{'name': 'example_resource_name_wide',\n   'timeseries_name': 'example_resource_name',\n   'shape': 'wide',\n   'format': 'csv.gz',\n   'filename': 'example_volume_v0.1.0_e001_example_resource_name_wide.csv.gz',\n   'hash': '5dc334942da3129e3dbfb9fbd02fe7042662b5d763929096adb571909ccf1d8a',\n   'content_hash': '7a80f4271ddae808da0d517deaeaab6cf0a484bb0002d38ff2c09d226e4e221a',\n   'profile': 'data-resource'},\n  {'name': 'example_resource_name_long',\n   'timeseries_name': 'example_resource_name',\n   'shape': 'long',\n   'format': 'csv.gz',\n   'filename': 'example_volume_v0.1.0_e001_example_resource_name_long.csv.gz',\n   'hash': 'f66123658ed668c563d69fbb5e9c9beae043dc777fe86fe544dd493953a91e59',\n   'content_hash': 'a08fa87c0f8d908b7f685c2bcc725b7736fc290cae94833b391157605508342c',\n   'profile': 'data-resource'}],\n 'profile': 'data-package'}</pre> <p>The metadata outlined above is available for clients to download and use for fetching the <code>Book</code>'s<code>Resources</code>. Upon deployment, the Book becomes immutable, meaning any modifications to its metadata or data necessitate the release of a new Book version. It is important to note that the steps provided herein pertain to the process of constructing a volume locally. This process does not cover the publication of the volume.</p>"},{"location":"how-to-guides/create_a_new_volume/#creating-a-new-volume","title":"Creating a new volume\u00b6","text":""},{"location":"how-to-guides/create_a_new_volume/#initial-setup","title":"Initial setup\u00b6","text":"<p>The first step for creating a new dataset is to create a new repository. By splitting datasets out into different repositories, we can ensure that the data release/management processes are much simpler. To make this step easier, we have created a copier repository that can be used to easily initialise a new repository using the following.</p> <pre><code>uvx copier copy gh:climate-resource/copier-bookshelf-dataset directory/to/new/repo\ncd directory/to/new/repo\ngit add .\ngit commit -m \"Initial commit\"\n</code></pre> <p>Copier will then ask you a few questions to set up the new repository. After the new repository is generated, a few more administration steps are required to get fully set up.</p> <ul> <li>A new repository should be created on GitHub</li> <li>The git remote <code>origin</code> should be updated to point to the new repository (<code>git remote add origin &lt;new-repo-ssh-url&gt;</code>)</li> <li>A <code>PERSONAL_ACCESS_TOKEN</code> secret is required to be added to the repository (instructions)</li> </ul> <p>After this, you can start creating your new dataset.</p>"},{"location":"how-to-guides/create_a_new_volume/#repository-structure","title":"Repository structure\u00b6","text":"<p>The repository structure is as follows:</p> <ul> <li>pyproject.toml: A description of the repository and its dependencies</li> <li>src/{dataset_name}.py: The main script that generates the dataset (this file includes a jupytext header)</li> <li>src/{dataset_name}.yaml: The metadata that describes the dataset and the versions that are to be processed.</li> </ul> <p>Some examples for source files for datasets can be found in the <code>notebooks</code> directory in this repository.</p>"},{"location":"how-to-guides/create_a_new_volume/#metadata-storage","title":"Metadata storage\u00b6","text":"<p>Updating the <code>{example_volume}.yaml</code> with the volume's metadata, this may include:</p> <ul> <li>name of the volume</li> <li>edition</li> <li>license</li> <li>metadata about author and author_email</li> <li>data dictionary</li> <li>detailed version information</li> <li>etc.</li> </ul>"},{"location":"how-to-guides/create_a_new_volume/#steps-in-processing-a-dataset","title":"Steps in processing a dataset\u00b6","text":"<p>Below the steps to process a dataset are described in more detail. These steps are included in the <code>src/{dataset_name}.py</code> file as a starting point, but can be modified as needed.</p>"},{"location":"how-to-guides/create_a_new_volume/#logging-configuration","title":"Logging configuration\u00b6","text":"<p>Load the packages and set up the basic configuration for logging:</p>"},{"location":"how-to-guides/create_a_new_volume/#metadata-loading","title":"Metadata loading\u00b6","text":"<p>If multiple versions are to be processed, the version can be passed as an argument to the script using a paper parameter section. Papermill will inject a new set of parameters into the notebook when it is run.</p> <pre><code>## %% tags=[\"parameters\"]\n# This cell contains additional parameters that are controlled using papermill\nlocal_bookshelf = tempfile.mkdtemp()\nversion = \"v3.4\"\n</code></pre>"},{"location":"how-to-guides/create_a_new_volume/#metadata-loading","title":"Metadata loading\u00b6","text":"<p>Load and verify the volume's metadata</p>"},{"location":"how-to-guides/create_a_new_volume/#data-loading-and-transformation","title":"Data loading and transformation\u00b6","text":"<p>Load the data intended for storage in the volume. This data may be sourced locally, scraped from the web, or downloaded from a server. For data downloads, we recommend using <code>pooch</code> to ensure integrity through hash verification.</p> <p>Once the data is loaded, perform any necessary manipulations to prepare it for storage. Convert the data to an <code>scmdata.ScmRun</code> object if it isn't already in this format.</p>"},{"location":"how-to-guides/create_a_new_volume/#local-book-creation","title":"Local book creation\u00b6","text":"<p>Initialize a local book instance using the prepared metadata:</p>"},{"location":"how-to-guides/create_a_new_volume/#resource-creation","title":"Resource creation\u00b6","text":"<p>Add a new <code>Resource</code> to the Book utilizing the <code>scmdata.ScmRun</code> object. This process involves copying the timeseries data into a local file, then calculating the hash of the file's contents to ensure data integrity. Additionally, the timeseries data is transformed into a long format, followed by a hash calculation of this transformed data. Utilizing these hashes allows for a straightforward verification process to determine if the files have undergone any modifications.</p>"},{"location":"how-to-guides/create_a_new_volume/#generation","title":"Generation\u00b6","text":"<p>The volumes for a book can be generated using:</p> <pre><code>make run\n</code></pre> <p>This will run the <code>src/{volume_name}.py</code> script for each version in the configuration file and output data to the <code>dist/</code> directory. The output folder contains the generated data, metadata, and the processed notebooks.</p> <p>The CI will automatically run this command during a Merge Request to verify that that processing scripts are valid.</p>"},{"location":"how-to-guides/find_all_books/","title":"Find all books","text":"In\u00a0[1]: Copied! <pre>from bookshelf import BookShelf\n\nshelf = BookShelf()\n\nvolume = \"rcmip-emissions\"\nshelf.list_versions(volume)\n</pre> from bookshelf import BookShelf  shelf = BookShelf()  volume = \"rcmip-emissions\" shelf.list_versions(volume) <pre>/home/runner/work/bookshelf/bookshelf/.venv/lib/python3.10/site-packages/scmdata/database/_database.py:9: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  import tqdm.autonotebook as tqdman\n</pre> Out[1]: <pre>['v5.1.0', 'v5.1.0']</pre>"},{"location":"how-to-guides/find_all_books/#finding-the-available-books","title":"Finding the available books\u00b6","text":"<p>The <code>bookshelf</code> library hosts a multitude of volumes, each with various versions or books. Below is a guide to discovering all available books.</p>"},{"location":"how-to-guides/find_all_books/#finding-all-available-books-of-a-specific-volume","title":"Finding all available books of a specific volume\u00b6","text":"<p>Should your interest lie in a particular volume, and you wish to explore all its available books in remote bookshelf, the <code>list_versions</code> function provided by <code>BookShelf</code> offers a straightforward solution.</p>"}]}